{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "40ygIDXEhjav"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import glob2\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from random import randrange\n",
    "import logging\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='./log.log',level=10, filemode = 'w', force=True, format='%(asctime)s   %(funcName)s - %(levelname)s:%(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXMhyoM3Wnq4"
   },
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_cat = r'C:\\Users\\dolek\\Documents\\GitHub\\Applied-Parallel-Programming\\Main\\Data\\Cat\\**'\n",
    "link_dog = r'C:\\Users\\dolek\\Documents\\GitHub\\Applied-Parallel-Programming\\Main\\Data\\Dog\\**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12499\n",
      "12499\n"
     ]
    }
   ],
   "source": [
    "n_samples = 50\n",
    "images  = []\n",
    "labels = []\n",
    "list_cat = glob2.glob(link_cat)\n",
    "print(len(list_cat))\n",
    "for i in range(n_samples):\n",
    "    if('jpg' in list_cat[i]):\n",
    "        img = Image.open(list_cat[i]).convert('RGB')\n",
    "        img = img.resize((400,400), Image.LANCZOS)\n",
    "        if len(np.array(img).shape)  == 3:\n",
    "            images.append(np.array(img))\n",
    "            labels.append(1)\n",
    "            \n",
    "list_dog = glob2.glob(link_dog)\n",
    "print(len(list_dog))\n",
    "for i in range(n_samples):\n",
    "    if('jpg' in list_dog[i]):\n",
    "        img = Image.open(list_dog[i]).convert('RGB')\n",
    "        img = img.resize((400,400), Image.LANCZOS)\n",
    "        if len(np.array(img).shape)  == 3:\n",
    "            images.append(np.array(img))\n",
    "            labels.append(-1)\n",
    "\n",
    "X = np.array(images)\n",
    "y = np.array(labels)\n",
    "\n",
    "for index in range(len(images)):\n",
    "    if images[index].shape[2] != 3:\n",
    "        print(index, images[index].shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oGqEcWO3NQs1"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([x.flatten() for x in X_train])\n",
    "x_test = np.array([x.flatten() for x in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import math\n",
    "@cuda.jit\n",
    "def init_K_kernel(X, gamma, n, m, K):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i >= n or j >= n:\n",
    "        return\n",
    "    s = 0.0\n",
    "    for v in range(m):\n",
    "        s -= gamma * (X[i, v] - X[j, v])**2\n",
    "    K[i, j] = math.exp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[211, 208, 199, ..., 210, 214, 215],\n",
       "       [  2,   2,   2, ...,  61,  70,  67],\n",
       "       [  3,   4,   0, ..., 207, 209,  97],\n",
       "       ...,\n",
       "       [196, 168, 146, ..., 182, 152, 118],\n",
       "       [234, 233, 247, ..., 179, 166, 157],\n",
       "       [ 67,  67,  67, ..., 172, 168, 165]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\cuda_program\\lib\\site-packages\\numba\\cuda\\dispatcher.py:488: NumbaPerformanceWarning: \u001b[1mGrid size 9 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 2.13691383e-02, 1.61910733e-02, ...,\n",
       "        2.62847399e-01, 1.08097442e-01, 1.07133180e-01],\n",
       "       [2.13691383e-02, 1.00000000e+00, 4.26474934e-02, ...,\n",
       "        4.74574821e-03, 2.31268122e-03, 2.94167889e-02],\n",
       "       [1.61910733e-02, 4.26474934e-02, 1.00000000e+00, ...,\n",
       "        4.46516756e-03, 9.88449956e-04, 1.29265647e-02],\n",
       "       ...,\n",
       "       [2.62847399e-01, 4.74574821e-03, 4.46516756e-03, ...,\n",
       "        1.00000000e+00, 1.58918309e-01, 9.81838495e-02],\n",
       "       [1.08097442e-01, 2.31268122e-03, 9.88449956e-04, ...,\n",
       "        1.58918309e-01, 1.00000000e+00, 3.44328065e-02],\n",
       "       [1.07133180e-01, 2.94167889e-02, 1.29265647e-02, ...,\n",
       "        9.81838495e-02, 3.44328065e-02, 1.00000000e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocksize = (32, 32)\n",
    "gridsize = (math.ceil(n/blocksize[0]), math.ceil(n/blocksize[1]))\n",
    "x_dev = cuda.to_device(x_train)\n",
    "K2 = np.empty((n, n),dtype=np.float64)\n",
    "k_dev = cuda.to_device(K2)\n",
    "init_K_kernel[gridsize, blocksize](x_dev, gamma, n, m, k_dev)\n",
    "K2 = k_dev.copy_to_host()\n",
    "K2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 1.79835880e-04, 2.20914763e-04, ...,\n",
       "        4.91775368e-07, 2.81803958e-06, 1.09368620e-04],\n",
       "       [1.14403108e-05, 1.00000000e+00, 1.42093704e-04, ...,\n",
       "        1.00748447e-04, 3.52486969e-05, 1.79542634e-06],\n",
       "       [1.01394729e-05, 1.27306714e-06, 1.00000000e+00, ...,\n",
       "        1.35160201e-05, 5.98178567e-05, 3.55660763e-06],\n",
       "       ...,\n",
       "       [1.19232344e-04, 2.01914677e-05, 1.81961246e-04, ...,\n",
       "        1.00000000e+00, 5.42534159e-06, 1.05779388e-05],\n",
       "       [7.37900329e-05, 2.34623440e-05, 2.34363302e-05, ...,\n",
       "        2.52624557e-05, 1.00000000e+00, 1.86390008e-05],\n",
       "       [4.15958620e-06, 1.53607579e-04, 3.36379680e-04, ...,\n",
       "        2.83653631e-05, 1.68657791e-05, 1.00000000e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = np.zeros((n,n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        K[i, j] = np.exp(-gamma * np.linalg.norm(x_train[i]-x_train[j])**2)\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 2.13691383e-02, 1.61910733e-02, ...,\n",
       "        2.62847399e-01, 1.08097442e-01, 1.07133180e-01],\n",
       "       [2.13691383e-02, 1.00000000e+00, 4.26474934e-02, ...,\n",
       "        4.74574821e-03, 2.31268122e-03, 2.94167889e-02],\n",
       "       [1.61910733e-02, 4.26474934e-02, 1.00000000e+00, ...,\n",
       "        4.46516756e-03, 9.88449956e-04, 1.29265647e-02],\n",
       "       ...,\n",
       "       [2.62847399e-01, 4.74574821e-03, 4.46516756e-03, ...,\n",
       "        1.00000000e+00, 1.58918309e-01, 9.81838495e-02],\n",
       "       [1.08097442e-01, 2.31268122e-03, 9.88449956e-04, ...,\n",
       "        1.58918309e-01, 1.00000000e+00, 3.44328065e-02],\n",
       "       [1.07133180e-01, 2.94167889e-02, 1.29265647e-02, ...,\n",
       "        9.81838495e-02, 3.44328065e-02, 1.00000000e+00]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmbuu/miniconda3/envs/cuda/lib/python3.10/site-packages/numba/cuda/dispatcher.py:539: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "blocksize = (32, 32)\n",
    "gridsize = (math.ceil(n/blocksize[0]), math.ceil(n/blocksize[1]))\n",
    "x_dev = cuda.to_device(x_train)\n",
    "k_dev = cuda.device_array((n,n))\n",
    "init_K_kernel[gridsize, blocksize](x_dev, gamma, n, m, k_dev)\n",
    "K2 = k_dev.copy_to_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 70,  72,  56, ..., 255, 255, 255],\n",
       "       [ 26,  15,  19, ...,  65,  53,  57]], dtype=uint8)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.00313437],\n",
       "       [0.00313437, 1.        ]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9444707392.000002"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(x_train[0]-x_train[1])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([x_train[0], x_train[1]])\n",
    "n, m = x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7516/465363959.py:3: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  s += (abs(x_train[0, i] - x_train[1, i])) **2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9444707392\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for i in range(m):\n",
    "    s += (abs(x_train[0, i] - x_train[1, i])) **2\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.00313437, 0.02896501, ..., 0.01328509, 0.0072313 ,\n",
       "        0.00554333],\n",
       "       [0.00313437, 1.        , 0.03486854, ..., 0.01607485, 0.00426829,\n",
       "        0.01650745],\n",
       "       [0.02896501, 0.03486854, 1.        , ..., 0.05423061, 0.03704297,\n",
       "        0.03899114],\n",
       "       ...,\n",
       "       [0.01328509, 0.01607485, 0.05423061, ..., 1.        , 0.03052409,\n",
       "        0.06569573],\n",
       "       [0.0072313 , 0.00426829, 0.03704297, ..., 0.03052409, 1.        ,\n",
       "        0.01068475],\n",
       "       [0.00554333, 0.01650745, 0.03899114, ..., 0.06569573, 0.01068475,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179.14580180035023"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.abs(K2-K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.00313437, 0.02896501, ..., 0.01328509, 0.0072313 ,\n",
       "        0.00554333],\n",
       "       [0.00313437, 1.        , 0.03486854, ..., 0.01607485, 0.00426829,\n",
       "        0.01650745],\n",
       "       [0.02896501, 0.03486854, 1.        , ..., 0.05423061, 0.03704297,\n",
       "        0.03899114],\n",
       "       ...,\n",
       "       [0.01328509, 0.01607485, 0.05423061, ..., 1.        , 0.03052409,\n",
       "        0.06569573],\n",
       "       [0.0072313 , 0.00426829, 0.03704297, ..., 0.03052409, 1.        ,\n",
       "        0.01068475],\n",
       "       [0.00554333, 0.01650745, 0.03899114, ..., 0.06569573, 0.01068475,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.00689827, 0.01148103, ..., 0.0027578 , 0.00282152,\n",
       "        0.01422989],\n",
       "       [0.00702531, 1.        , 0.00863966, ..., 0.01815422, 0.00165954,\n",
       "        0.00803732],\n",
       "       [0.00397591, 0.01441174, 1.        , ..., 0.00283153, 0.01175133,\n",
       "        0.00377082],\n",
       "       ...,\n",
       "       [0.01792932, 0.00414395, 0.0169288 , ..., 1.        , 0.00290669,\n",
       "        0.00568302],\n",
       "       [0.01695725, 0.01790866, 0.01317853, ..., 0.02838869, 1.        ,\n",
       "        0.01240341],\n",
       "       [0.00415244, 0.00526936, 0.01674873, ..., 0.01305657, 0.00507525,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1,  1, -1, -1, -1, -1,  1, -1,  1,  1,  1,  1,  1,  1, -1,  1,\n",
       "        1,  1,  1, -1,  1, -1,  1,  1,  1,  1,  1, -1, -1, -1, -1,  1,  1,\n",
       "       -1,  1,  1, -1,  1,  1,  1,  1,  1,  1, -1, -1,  1,  1,  1,  1, -1,\n",
       "        1, -1,  1, -1, -1, -1, -1, -1,  1,  1, -1,  1,  1, -1,  1,  1, -1,\n",
       "       -1, -1, -1, -1, -1,  1,  1, -1,  1, -1, -1,  1, -1,  1, -1,  1,  1,\n",
       "       -1,  1, -1, -1,  1,  1,  1, -1, -1,  1, -1,  1, -1, -1, -1,  1,  1,\n",
       "       -1, -1, -1, -1, -1, -1,  1, -1,  1, -1,  1, -1,  1, -1,  1, -1, -1,\n",
       "        1,  1,  1,  1,  1,  1, -1, -1, -1, -1,  1, -1,  1, -1,  1,  1, -1,\n",
       "        1,  1, -1,  1, -1,  1, -1,  1,  1, -1, -1,  1, -1, -1,  1,  1,  1,\n",
       "        1, -1,  1, -1, -1, -1, -1,  1,  1, -1, -1, -1,  1,  1,  1, -1, -1,\n",
       "        1, -1,  1, -1, -1,  1,  1,  1, -1, -1, -1,  1,  1,  1, -1, -1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1,  1, -1, -1, -1,  1, -1,  1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        1,  1,  1,  1,  1,  1, -1,  1, -1,  1, -1,  1, -1, -1, -1, -1,  1,\n",
       "        1,  1,  1,  1,  1, -1, -1, -1, -1,  1, -1,  1, -1, -1, -1, -1,  1,\n",
       "        1,  1, -1, -1, -1,  1, -1, -1,  1,  1, -1, -1,  1,  1,  1, -1,  1,\n",
       "       -1,  1, -1,  1,  1,  1, -1,  1, -1,  1,  1,  1, -1, -1, -1,  1,  1,\n",
       "        1, -1,  1,  1, -1, -1, -1, -1, -1,  1,  1, -1, -1,  1, -1,  1, -1,\n",
       "        1, -1,  1, -1,  1, -1, -1,  1, -1,  1, -1, -1, -1, -1,  1,  1, -1,\n",
       "       -1,  1,  1,  1,  1, -1, -1,  1,  1, -1,  1, -1, -1,  1,  1,  1,  1,\n",
       "        1,  1, -1,  1, -1, -1, -1, -1, -1,  1])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "  def __init__(self, C=1.0, gamma=0.1, tol=1e-3, max_iter=1200):\n",
    "    self.C = C\n",
    "    self.gamma = gamma\n",
    "    self.tol = tol\n",
    "    self.max_iter = max_iter\n",
    "    self.n_iter = 0\n",
    "    self.eps = 1e-5\n",
    "\n",
    "  def rbf(self, x1, x2):\n",
    "    return np.exp(-self.gamma * (np.linalg.norm(x1 - x2) **2 ))\n",
    "  \n",
    "  def output(self, x, X, y, n_samples):\n",
    "    return np.sum([self.alphas[i] * y[i] * self.rbf(X[i], x) for i in range(n_samples)]) - self.b\n",
    "\n",
    "  def get_error(self, i, X, y):\n",
    "    if self.non_bound[i]:\n",
    "      return self.errors[i]\n",
    "    else:\n",
    "      op = self.output(X[i], X, y, self.n_samples) - y[i]\n",
    "      self.errors[i] = op\n",
    "      return op  \n",
    "\n",
    "  def predict(self, X):\n",
    "    pred = []\n",
    "    for x in X:\n",
    "      pred.append(np.sign(self.output(x, self.support_vectors, self.support_vector_labels, self.n_vector)))\n",
    "    return pred\n",
    "    \n",
    "  def compute_L_H(self):\n",
    "    if self.y1 != self.y2:\n",
    "      L = max(0, self.alpha2 - self.alpha1)\n",
    "      H = min(self.C, self.C + self.alpha2 - self.alpha1)\n",
    "    else:\n",
    "      L = max(0, self.alpha2 + self.alpha1 - self.C)\n",
    "      H = min(self.C, self.alpha2 + self.alpha1)\n",
    "    return L, H\n",
    "  \n",
    "  def compute_threshold(self, alpha1_new, alpha2_new, k11, k12, k22):\n",
    "    b1 = self.E1 + self.y1 * (alpha1_new - self.alpha1) * k11 + \\\n",
    "        self.y2 * (alpha2_new - self.alpha2) * k12 + self.b\n",
    "    b2 = self.E2 + self.y1 * (alpha1_new - self.alpha1) * k12 + \\\n",
    "        self.y2 * (alpha2_new - self.alpha2) * k22 + self.b\n",
    "\n",
    "    if 0 < alpha1_new and alpha1_new < self.C:\n",
    "      return b1\n",
    "    if 0 < alpha2_new and alpha2_new < self.C:\n",
    "      return b2\n",
    "    return (b1 + b2)/2.0\n",
    "    \n",
    "  def update_error_cache(self, alpha1_new, alpha2_new, old_b, X):\n",
    "    delta1 = self.y1 * (alpha1_new - self.alpha1)\n",
    "    delta2 = self.y2 * (alpha2_new - self.alpha2)\n",
    "    for i in self.non_bound_idx:\n",
    "      self.errors[i] += delta1 * self.rbf(self.x1, X[i]) + \\\n",
    "                        delta2 * self.rbf(self.x2, X[i]) + \\\n",
    "                        old_b - self.b\n",
    "                        \n",
    "  def update_error2(self, alpha1_new, alpha2_new, old_b, X):\n",
    "    delta1 = self.y1 * (alpha1_new - self.alpha1)\n",
    "    delta2 = self.y2 * (alpha2_new - self.alpha2)\n",
    "    for i in range(self.n_samples):\n",
    "      self.errors2[i] += delta1 * self.rbf(self.x1, X[i]) + \\\n",
    "                        delta2 * self.rbf(self.x2, X[i]) + \\\n",
    "                        old_b - self.b\n",
    "    \n",
    "  def update_non_bound(self, i1, i2):\n",
    "    if 0 < self.alphas[i1] and self.alphas[i1] < self.C:\n",
    "      self.non_bound[i1] = True\n",
    "      if i1 not in self.non_bound_idx:\n",
    "        self.non_bound_idx.append(i1)\n",
    "    else:\n",
    "      self.non_bound[i1] = False\n",
    "      if i1 in self.non_bound_idx:\n",
    "        self.non_bound_idx.remove(i1)\n",
    "      \n",
    "    if 0 < self.alphas[i2] and self.alphas[i2] < self.C:\n",
    "      self.non_bound[i2] = True\n",
    "      if i2 not in self.non_bound_idx:\n",
    "        self.non_bound_idx.append(i2)\n",
    "    else:\n",
    "      self.non_bound[i2] = False\n",
    "      if i2 in self.non_bound_idx:\n",
    "        self.non_bound_idx.remove(i2)\n",
    "\n",
    "    \n",
    "  def get_alpha2(self, L, H, k11, k12, k22, s):\n",
    "    eta = k11 + k22 - 2 * k12\n",
    "    if eta > 0:\n",
    "      alpha2_new = self.alpha2 + self.y2 * (self.E1-self.E2)/eta\n",
    "      alpha2_new = min(H, max(L, alpha2_new))\n",
    "    else:\n",
    "      # print(\"ETA < 0\")\n",
    "      # logging.error(\"ETA <= 0\")\n",
    "      f1 = self.y1*(self.E1 + self.b) - self.alpha1*k11 - s*self.alpha2*k12\n",
    "      f2 = self.y2*(self.E2 + self.b) - s*self.alpha1*k12 - self.alpha2*k22\n",
    "      L1 = self.alpha1 + s*(self.alpha2-L)\n",
    "      H1 = self.alpha1 + s*(self.alpha2-H)\n",
    "      Lobj = L1*f1 + L*f2 + 0.5*(L1**2)*k11 + 0.5*(L**2)*k22 + s*L*L1*k12\n",
    "      Hobj = H1*f1 + H*f2 + 0.5*(H1**2)*k11 + 0.5*(H**2)*k22 + s*H*H1*k12\n",
    "      if Lobj < Hobj - self.eps:\n",
    "        alpha2_new = L\n",
    "      elif Lobj > Hobj + self.eps:\n",
    "        alpha2_new = H\n",
    "      else:\n",
    "        alpha2_new = self.alpha2\n",
    "    return alpha2_new\n",
    "    \n",
    "  def take_step(self, i1, i2, X, y):\n",
    "    if i1 == i2:\n",
    "      return 0\n",
    "    self.y2 = y[i2]\n",
    "    self.alpha2 = self.alphas[i2]\n",
    "    self.x2 = X[i2]\n",
    "    self.E2 = self.get_error(i2, X, y)\n",
    "    s = self.y1 * self.y2\n",
    "    L, H = self.compute_L_H()\n",
    "    if L==H:\n",
    "      # logging.debug(\"L=H\")\n",
    "      return 0\n",
    "    k11 = self.rbf(self.x1, self.x1)\n",
    "    k12 = self.rbf(self.x1, self.x2)\n",
    "    k22 = self.rbf(self.x2, self.x2)\n",
    "    self.alphas[i2] = self.get_alpha2(L, H, k11, k12, k22, s)\n",
    "    # logging.debug(f\"{self.alphas[i2]-self.alpha2}\\t{self.eps * (self.alphas[i2] + self.alpha2 + self.eps)}\")\n",
    "    if abs(self.alphas[i2] -self.alpha2) < self.eps * (self.alphas[i2] + self.alpha2 + self.eps):\n",
    "      return 0\n",
    "    self.alphas[i1] = self.alpha1 + s * (self.alpha2 - self.alphas[i2])\n",
    "    old_b = self.b\n",
    "    self.b = self.compute_threshold(self.alphas[i1], self.alphas[i2], k11, k12, k22)\n",
    "    self.update_non_bound(i1, i2)\n",
    "    self.update_error_cache(self.alphas[i1], self.alphas[i2], old_b, X)\n",
    "    # logging.warning(f\"alpha[{i1}]: {self.alphas[i1]};\\talpha[{i2}]: {self.alphas[i2]}\") #;\\t[{L},  {H}]\")\n",
    "    return 1\n",
    "  \n",
    "  def second_choice_heuristic(self, X ,y):\n",
    "    i2 = -1\n",
    "    m = 0.0\n",
    "    for i in self.non_bound_idx:\n",
    "      step = abs(self.get_error(i, X, y) - self.E1)\n",
    "      if step > m:\n",
    "        m = step\n",
    "        i2 = i\n",
    "    return i2\n",
    "  \n",
    "  def init_K(self, ):\n",
    "    f\n",
    "  def examine_example(self, i1, X, y):\n",
    "    self.y1 = y[i1]\n",
    "    self.alpha1 = self.alphas[i1]\n",
    "    self.x1 = X[i1]\n",
    "    self.E1 = self.get_error(i1, X, y)\n",
    "    \n",
    "    r2 = self.E1 * self.y1\n",
    "    if (r2 < -self.tol and self.alpha1 < self.C) or \\\n",
    "      (r2 > self.tol and self.alpha1 > 0):\n",
    "      n_non_bound = len(self.non_bound_idx)\n",
    "      mark = -1\n",
    "      if (n_non_bound > 1):\n",
    "        i2 = self.second_choice_heuristic(X, y)\n",
    "        if i2 != -1:\n",
    "          if self.take_step(i1, i2, X, y):\n",
    "            return 1\n",
    "          # logging.info(f\"B1 Fail with {i1}, {i2}\")\n",
    "        mark = i2\n",
    "      if n_non_bound > 0:\n",
    "        rand_i = randrange(n_non_bound)\n",
    "        for i2 in self.non_bound_idx[rand_i:] + self.non_bound_idx[:rand_i]:\n",
    "          if i2 == mark:\n",
    "            continue\n",
    "          if self.take_step(i1, i2, X, y):\n",
    "            return 1\n",
    "          # logging.info(f\"B2 Fail with {i1}, {i2}\")\n",
    "          \n",
    "      rand_i = randrange(self.n_samples)\n",
    "      all_indexes = list(range(self.n_samples))\n",
    "      for i2 in all_indexes[rand_i:] + all_indexes[:rand_i]:\n",
    "        if self.non_bound[i2]:\n",
    "          continue\n",
    "        if self.take_step(i1, i2, X, y):\n",
    "          return 1\n",
    "        # logging.info(f\"B3 Fail with {i1}, {i2}\")\n",
    "    return 0\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    random.seed(42)\n",
    "    self.n_samples, self.n_features = X.shape\n",
    "    self.errors = np.zeros(self.n_samples)\n",
    "    self.alphas = np.zeros(self.n_samples)\n",
    "    self.non_bound = np.array([False for _ in range(self.n_samples)])\n",
    "    self.non_bound_idx = []\n",
    "    self.errors2 = -1.0*y\n",
    "    logging.info(y)\n",
    "    logging.info(self.errors2)\n",
    "    self.b = 0\n",
    "    num_changed = 0\n",
    "    examine_all = True\n",
    "    self.n_iter = 0\n",
    "    while num_changed > 0 or examine_all:\n",
    "      self.n_iter += 1\n",
    "      # logging.debug(f\"N iter: {self.n_iter}\")\n",
    "      # logging.debug(f\"examine_all: {examine_all}, numchanged: {num_changed}\")\n",
    "      num_changed = 0\n",
    "      if examine_all:\n",
    "        for i in range(self.n_samples):\n",
    "          num_changed += self.examine_example(i, X, y)\n",
    "      else:\n",
    "        for i in range(self.n_samples):\n",
    "          if 0 < self.alphas[i] < self.C:\n",
    "            num_changed += self.examine_example(i, X, y)\n",
    "  \n",
    "      if examine_all:\n",
    "        examine_all = False\n",
    "      elif num_changed == 0:\n",
    "        examine_all = True\n",
    "        \n",
    "    sv_idx = (self.alphas > 0)\n",
    "    logging.info(f\"Filtering support vectors, there are {np.sum(sv_idx)} alphas\")\n",
    "    self.support_vectors = X[sv_idx]\n",
    "    self.support_vector_labels = y[sv_idx]\n",
    "    self.n_vector = np.sum(sv_idx)\n",
    "    logging.info(\"Done fitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPX7eu3l_--N",
    "outputId": "5c4d5586-0b7f-4f40-b357-58e80195bdfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5666666666666667"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVM(gamma = 1/(x_train.shape[1]*x_train.var()))\n",
    "svm.fit(x_train, y_train)\n",
    "pred = svm.predict(x_test)\n",
    "accuracy_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83506981, 0.83042563, 0.95350821, 0.54854726, 0.59280639,\n",
       "       1.        , 0.93562079, 0.90069006, 0.85231796, 1.        ,\n",
       "       1.        , 0.8612385 , 1.        , 0.6689005 , 1.        ,\n",
       "       0.89544531, 0.47865184, 0.7685695 , 0.79926202, 0.85835592,\n",
       "       1.        , 0.91951677, 0.74384491, 0.95292311, 1.        ,\n",
       "       1.        , 0.86264301, 1.        , 0.71628773, 1.        ,\n",
       "       0.8173455 , 1.        , 0.95200105, 0.97979352, 1.        ,\n",
       "       1.        , 1.        , 0.84260766, 0.79387083, 0.85681705,\n",
       "       0.89734103, 0.95698785, 0.930525  , 1.        , 1.        ,\n",
       "       0.66855641, 0.98609099, 0.75380385, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.85166502, 0.87574812, 0.75944717,\n",
       "       1.        , 0.90525173, 0.96674735, 0.94367203, 0.87471162,\n",
       "       1.        , 0.9476128 , 1.        , 0.90832863, 0.7732029 ,\n",
       "       1.        , 0.84005516, 0.98547386, 0.92549059, 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.75308131, 0.88190737,\n",
       "       1.        , 0.71765381, 1.        , 0.62685192, 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.8351922 , 1.        ,\n",
       "       0.75374293, 0.83345238, 1.        , 1.        , 0.93981581,\n",
       "       0.85956787, 0.85348758, 1.        , 1.        , 1.        ,\n",
       "       0.65927616, 1.        , 1.        , 0.8075043 , 1.        ,\n",
       "       1.        , 1.        , 0.62623241, 1.        , 0.53860248,\n",
       "       1.        , 0.97512781, 0.93846138, 0.97031248, 0.85403596,\n",
       "       0.9761615 , 1.        , 0.8857741 , 0.93606132, 0.88072088,\n",
       "       1.        , 1.        , 0.98051881, 0.44081381, 1.        ,\n",
       "       1.        , 0.91411802, 1.        , 0.87082232, 0.9201974 ,\n",
       "       0.88468881, 1.        , 0.84701425, 1.        , 1.        ,\n",
       "       1.        , 0.83808314, 0.72978052, 0.51602346, 1.        ,\n",
       "       1.        , 0.88653713, 1.        , 1.        , 0.92206596,\n",
       "       0.70785178, 1.        , 0.6579316 , 1.        , 1.        ,\n",
       "       0.98224988, 0.92345865, 0.83151756, 0.93619301, 0.97303899,\n",
       "       0.79723524, 0.88764523, 0.95847737, 1.        , 1.        ,\n",
       "       1.        , 0.36527341, 0.97031894, 1.        , 0.89940663,\n",
       "       1.        , 0.84329618, 0.94131717, 0.91801363, 0.99085769,\n",
       "       0.81377756, 1.        , 0.77024077, 1.        , 0.90946552,\n",
       "       0.96826276, 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.90393786, 1.        , 0.94983477, 0.84734449, 1.        ,\n",
       "       0.98672489, 0.93551689, 0.9796231 , 1.        , 1.        ,\n",
       "       1.        , 0.9660823 , 0.70334162, 1.        , 1.        ,\n",
       "       0.89289228, 1.        , 0.92635213, 1.        , 1.        ,\n",
       "       0.96985162, 0.90508601, 1.        , 1.        , 0.30953908,\n",
       "       1.        , 0.99758796, 1.        , 0.98022118, 1.        ,\n",
       "       0.91304485, 0.93051252, 0.95564771, 0.8945584 , 0.93164154,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.86241516,\n",
       "       0.94842543, 0.79635705, 0.84411126, 0.94423281, 1.        ,\n",
       "       1.        , 0.81140955, 0.86697908, 0.95339657, 0.959327  ,\n",
       "       1.        , 0.9150158 , 0.64471284, 1.        , 1.        ,\n",
       "       1.        , 0.71323935, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.44580553, 1.        ,\n",
       "       0.92800794, 0.99350498, 0.92082659, 0.88109491, 0.91056937,\n",
       "       1.        , 1.        , 1.        , 0.9965853 , 0.97306936,\n",
       "       0.8343607 , 0.80567994, 1.        , 1.        , 0.70979962,\n",
       "       0.8904209 , 1.        , 1.        , 1.        , 0.79041071,\n",
       "       0.45047475, 1.        , 0.87006144, 0.82192258, 0.90314958,\n",
       "       1.        , 0.9819484 , 1.        , 0.89847651, 1.        ,\n",
       "       0.61323322, 0.81477492, 1.        , 0.79558565, 1.        ,\n",
       "       0.93974619, 1.        , 0.9104689 , 1.        , 1.        ,\n",
       "       0.99540604, 1.        , 0.74473084, 0.761043  , 1.        ,\n",
       "       1.        , 0.94685943, 1.        , 0.4175418 , 0.97713068,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.86479755,\n",
       "       0.77695092, 1.        , 0.97666917, 0.9219466 , 0.63067352,\n",
       "       0.54348562, 1.        , 0.74654947, 1.        , 1.        ,\n",
       "       0.80182937, 0.51435611, 0.88591766, 0.94611052, 0.50493188,\n",
       "       0.90021367, 1.        , 1.        , 0.91075074, 1.        ,\n",
       "       0.77270768, 0.8159957 , 0.8891507 , 1.        , 0.9765848 ,\n",
       "       0.96932382, 1.        , 1.        , 1.        , 0.96652513,\n",
       "       0.92855897, 1.        , 1.        , 0.9619374 , 0.93933076,\n",
       "       0.43265592, 0.88492512, 0.82002842, 1.        , 0.88538786,\n",
       "       1.        , 1.        , 0.76138318, 0.84411174, 0.88280471,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.9518639 ])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpmSnc60Aa8F",
    "outputId": "9f9be904-2184-40d2-9baf-a4b84dcb399b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6066666666666667"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_sklearn = np.array([str(y) for y in y_train])\n",
    "y_test_sklearn = np.array([str(y) for y in y_test])\n",
    "\n",
    "svc = SVC(kernel='rbf', gamma = 1/(x_train.shape[1]*x_train.var()))\n",
    "svc.fit(x_train, y_train_sklearn)\n",
    "pred = svc.predict(x_test)\n",
    "accuracy_score(pred, y_test_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        , -0.3512697 , -0.74366543, -1.        , -1.        ,\n",
       "        -0.90048824, -0.89481568, -1.        , -1.        , -1.        ,\n",
       "        -0.63694747, -1.        , -1.        , -1.        , -0.32147536,\n",
       "        -1.        , -1.        , -0.317044  , -1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        , -0.43088005, -0.6980635 ,\n",
       "        -1.        , -0.22184769, -0.83409572, -0.70396823, -0.6314654 ,\n",
       "        -1.        , -1.        , -1.        , -0.616992  , -1.        ,\n",
       "        -0.50747575, -1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -1.        , -0.79985626, -0.95829998, -1.        ,\n",
       "        -1.        , -1.        , -0.82584718, -0.57320258, -1.        ,\n",
       "        -1.        , -1.        , -0.88297668, -1.        , -1.        ,\n",
       "        -1.        , -0.96089   , -1.        , -1.        , -0.62601639,\n",
       "        -1.        , -1.        , -1.        , -0.72495606, -1.        ,\n",
       "        -1.        , -0.67879329, -1.        , -0.67715066, -0.69154329,\n",
       "        -1.        , -1.        , -0.19719961, -1.        , -1.        ,\n",
       "        -0.81941237, -1.        , -1.        , -1.        , -0.67818225,\n",
       "        -1.        , -1.        , -0.19644   , -1.        , -1.        ,\n",
       "        -1.        , -0.78081806, -1.        , -1.        , -1.        ,\n",
       "        -1.        , -0.57680738, -1.        , -0.49390523, -1.        ,\n",
       "        -1.        , -1.        , -1.        , -0.91945145, -1.        ,\n",
       "        -0.7132931 , -1.        , -0.98483673, -1.        , -1.        ,\n",
       "        -0.84089626, -1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        , -0.84702872, -1.        ,\n",
       "        -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -0.5743284 , -1.        , -0.76735297, -1.        ,\n",
       "        -0.98025359, -1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        , -1.        , -0.81632482,\n",
       "        -1.        , -0.60987003, -1.        , -1.        , -1.        ,\n",
       "        -0.47372941, -1.        , -1.        , -1.        , -0.91456903,\n",
       "        -1.        , -1.        , -0.93312406, -1.        , -0.78909692,\n",
       "        -0.73195413, -0.12695299, -1.        , -1.        , -1.        ,\n",
       "        -1.        , -0.50341153, -1.        , -1.        , -1.        ,\n",
       "        -1.        , -0.64501638, -1.        , -1.        , -1.        ,\n",
       "        -1.        , -0.92304298, -1.        , -1.        , -1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.99304467,\n",
       "         1.        ,  0.91214708,  1.        ,  1.        ,  1.        ,\n",
       "         0.69787056,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.92377448,  1.        ,  1.        ,  0.75907118,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.13472273,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.95348772,  1.        ,  1.        ,\n",
       "         1.        ,  0.64913098,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.65195587,  0.59463891,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.08641927,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.95605342,  1.        ,  1.        ,  0.44906778,  1.        ,\n",
       "         0.61042251,  1.        ,  0.55010086,  0.62895096,  0.37500449,\n",
       "         0.87133916,  1.        ,  1.        ,  0.09212249,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.44778771,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.62970464,  0.64227912,  0.99824492,  1.        ,  0.99677228,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99904163,  1.        ,  0.67503076,\n",
       "         1.        ,  0.29563878,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.47561497,  0.904149  ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.94496062,  0.22575001,\n",
       "         1.        ,  1.        ,  0.52630168,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.83630457,  1.        ,  0.78937016,  0.83916   ,  0.02735   ,\n",
       "         1.        ,  1.        ,  0.77215066,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.78977367,  1.        ,\n",
       "         1.        ,  0.6190398 ,  0.52537548,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.88873397,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.4768313 ,  0.83263417]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.dual_coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_simplified:\n",
    "    def __init__(self, C=1.0, gamma=0.1, tol=1e-3, max_iter=1200):\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.alphas = None\n",
    "        self.b = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.n_iter_ = 0\n",
    "\n",
    "    def rbf_kernel(self, x1, x2):\n",
    "      return np.exp(-self.gamma * (np.linalg.norm(x1 - x2) ** 2))\n",
    "    \n",
    "    def compute_error(self, n_samples, y, i, K):\n",
    "      err = self.b - y[i]\n",
    "      for j in range(n_samples):\n",
    "        err += self.alphas[j] * y[j] * K[j, i]\n",
    "      return err\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "      # random.seed(42)\n",
    "      n_samples, n_features = X.shape\n",
    "\n",
    "      K = np.zeros((n_samples, n_samples))\n",
    "      for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "          K[i,j] = self.rbf_kernel(X[i], X[j])\n",
    "      \n",
    "      self.alphas = np.zeros(n_samples)\n",
    "      self.b = 0\n",
    "      passes = 0\n",
    "      while passes < self.max_iter:\n",
    "        self.n_iter_ += 1\n",
    "        num_changed_alphas = 0\n",
    "        for i in range(n_samples):\n",
    "          Ei = np.sum(self.alphas*y*K[i,:]) + self.b - y[i]\n",
    "          # Ei = self.compute_error(n_samples, y, i, K)\n",
    "          # Ei = np.sum(np.dot(np.dot(self.alphas, y), K[i,:])) +self.b - y[i]\n",
    "          # if abs(ei - Ei)>0.00001:\n",
    "          #   print(Ei, ei)\n",
    "          if (((y[i]*Ei < -self.tol) and (self.alphas[i] < self.C)) or ((y[i]*Ei>self.tol) and (self.alphas[i]>0))):\n",
    "            j = random.choice(list(range(i))+list(range(i+1,n_samples)))\n",
    "            Ej = np.sum(self.alphas*y*K[j, :]) + self.b - y[j]\n",
    "            # Ej = self.compute_error(n_samples, y, j, K)\n",
    "            # Ej = np.sum(np.dot(np.dot(self.alphas, y), K[j,:])) +self.b - y[j]\n",
    "            alpha_i_old = self.alphas[i]\n",
    "            alpha_j_old = self.alphas[j]\n",
    "            if (y[i]!=y[j]):\n",
    "              L = max(0, self.alphas[j] - self.alphas[i])\n",
    "              H = min(self.C, self.C + self.alphas[j] - self.alphas[i])\n",
    "            else:\n",
    "              L = max(0, self.alphas[i] + self.alphas[j] - self.C)\n",
    "              H = min(self.C, self.alphas[i] + self.alphas[j])\n",
    "            if L==H:\n",
    "              continue\n",
    "            eta = 2 * K[i,j] - K[i,i] - K[j,j]\n",
    "            if eta>=0:\n",
    "              continue\n",
    "            self.alphas[j] -= y[j]*(Ei-Ej)/eta\n",
    "            self.alphas[j] = max(L, min(H, self.alphas[j]))\n",
    "\n",
    "            if (abs(self.alphas[j] - alpha_j_old) < 0.00001):\n",
    "              continue\n",
    "            self.alphas[i] += y[i]*y[j]*(alpha_j_old - self.alphas[j])\n",
    "            \n",
    "            b1 = self.b - Ei - y[i]*(self.alphas[i]-alpha_i_old)*K[i,i] - y[j]*(self.alphas[j]-alpha_j_old)*K[i,j]\n",
    "            b2 = self.b - Ej - y[i]*(self.alphas[i]-alpha_i_old)*K[i,j] - y[j]*(self.alphas[j]-alpha_j_old)*K[j,j]\n",
    "            if (0 < self.alphas[i] and self.alphas[i]<self.C):\n",
    "              self.b = b1\n",
    "            elif (0<self.alphas[j] and self.alphas[j]<self.C):\n",
    "              self.b = b2\n",
    "            else:\n",
    "              self.b = (b1 + b2)/2\n",
    "            num_changed_alphas += 1\n",
    "        if (num_changed_alphas == 0):\n",
    "          passes+= 1\n",
    "        else:\n",
    "          passes = 0\n",
    "      sv_idx = (self.alphas>0)\n",
    "      self.sv_idx = sv_idx\n",
    "      self.alphas = self.alphas[sv_idx]\n",
    "      self.support_vectors = X[sv_idx]\n",
    "      self.support_vector_labels = y[sv_idx]\n",
    "    def predict(self, X):\n",
    "      y_pred = np.zeros(len(X))\n",
    "      for i in range(len(X)):\n",
    "        s = self.b\n",
    "        for alpha, sv_y, sv_x in zip(self.alphas, self.support_vector_labels, self.support_vectors):\n",
    "          s += alpha * sv_y * self.rbf_kernel(X[i], sv_x)\n",
    "        y_pred[i] = s\n",
    "      return np.sign(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVM_simplified(gamma = 1/(x_train.shape[1]*x_train.var()))\n",
    "svm.fit(x_train, y_train)\n",
    "pred = svm.predict(x_test)\n",
    "accuracy_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.92762639, 0.82906063, 0.94458047, 0.69172372, 0.986416  ,\n",
       "       0.96763194, 1.        , 0.97107458, 1.        , 0.91516803,\n",
       "       1.        , 0.92574395, 1.        , 1.        , 1.        ,\n",
       "       0.70697383, 1.        , 1.        , 0.98896341, 0.81702256,\n",
       "       0.76334599, 1.        , 0.82132154, 1.        , 0.46493925,\n",
       "       1.        , 0.60005414, 1.        , 0.89364161, 0.62531251,\n",
       "       0.83469202, 0.79022269, 1.        , 0.98592688, 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.8875843 , 1.        , 0.77932138, 0.84171277,\n",
       "       0.98136109, 0.91538332, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.89488124, 1.        , 0.97383139,\n",
       "       0.83638835, 0.86499324, 0.94779587, 0.90457653, 0.89512355,\n",
       "       1.        , 1.        , 0.94189352, 1.        , 0.81649489,\n",
       "       0.99864246, 1.        , 0.98399531, 1.        , 0.93090377,\n",
       "       0.98438269, 1.        , 1.        , 1.        , 0.96213614,\n",
       "       1.        , 0.95541644, 1.        , 1.        , 0.81327049,\n",
       "       0.87914604, 1.        , 0.96556653, 0.73652385, 0.72615956,\n",
       "       0.92843019, 1.        , 0.96288926, 0.90543766, 0.94550008,\n",
       "       1.        , 1.        , 0.79610262, 1.        , 1.        ,\n",
       "       0.86462886, 0.93682681, 1.        , 1.        , 1.        ,\n",
       "       0.88028212, 1.        , 0.8924039 , 0.8659519 , 1.        ,\n",
       "       1.        , 0.73419689, 0.98066944, 1.        , 1.        ,\n",
       "       1.        , 0.97791428, 1.        , 1.        , 1.        ,\n",
       "       0.85511177, 1.        , 1.        , 1.        , 0.95660175,\n",
       "       1.        , 1.        , 1.        , 0.99760422, 0.91827125,\n",
       "       0.91265677, 1.        , 1.        , 0.90334901, 0.97616653,\n",
       "       1.        , 0.90541686, 0.89203106, 0.51138477, 1.        ,\n",
       "       1.        , 0.93965064, 0.9762014 , 0.83999261, 0.9070609 ,\n",
       "       1.        , 1.        , 1.        , 0.92010567, 0.92823689,\n",
       "       1.        , 0.87355638, 0.93559682, 0.79874209, 1.        ,\n",
       "       0.90565535, 1.        , 0.9876908 , 0.78872057, 1.        ,\n",
       "       0.82333013, 1.        , 1.        , 0.97366751, 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.89219326,\n",
       "       0.95574423, 0.84832566, 1.        , 0.49987839, 1.        ,\n",
       "       1.        , 0.72262429, 1.        , 1.        , 0.65920398,\n",
       "       1.        , 1.        , 0.99750934, 0.86649366, 0.89325041,\n",
       "       0.97183444, 1.        , 1.        , 1.        , 0.96312254,\n",
       "       1.        , 0.91853387, 1.        , 1.        , 1.        ,\n",
       "       0.97699313, 1.        , 0.81089057, 0.96286835, 1.        ,\n",
       "       0.79643387, 0.94783552, 0.78061366, 1.        , 1.        ,\n",
       "       1.        , 0.87075299, 1.        , 1.        , 0.68880756,\n",
       "       0.80833004, 1.        , 0.55848925, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.78151572,\n",
       "       1.        , 1.        , 1.        , 0.92149804, 0.96167688,\n",
       "       0.86993235, 1.        , 1.        , 0.97825478, 0.76698228,\n",
       "       0.88182937, 1.        , 0.90359969, 0.99736158, 0.98013943,\n",
       "       0.99759861, 0.94894517, 0.75492909, 0.69231982, 0.77695315,\n",
       "       0.87733277, 0.82288449, 0.7950365 , 0.99481181, 1.        ,\n",
       "       0.65309435, 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.73670976, 0.95509622, 0.90523805, 1.        , 0.96633965,\n",
       "       1.        , 1.        , 0.95552602, 0.78906367, 1.        ,\n",
       "       1.        , 0.82972015, 1.        , 0.86786604, 0.93667316,\n",
       "       1.        , 0.97247591, 1.        , 1.        , 1.        ,\n",
       "       0.79383543, 0.99662154, 0.77153005, 1.        , 0.98297331,\n",
       "       1.        , 1.        , 0.86331038, 1.        , 1.        ,\n",
       "       0.90323047, 0.83989594, 1.        , 0.93439271, 0.66483352,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.9425021 ,\n",
       "       1.        , 0.8002859 , 0.92426625, 1.        , 0.96961605,\n",
       "       1.        , 0.74271658, 0.82607526, 0.79148103, 1.        ,\n",
       "       0.65202046, 0.83387217, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.82375352, 1.        , 0.91203802, 0.46334175,\n",
       "       0.90572685, 1.        , 0.89617297, 1.        , 1.        ,\n",
       "       1.        , 0.89676569, 1.        , 1.        , 0.61239375,\n",
       "       0.88175532, 0.95949798, 1.        , 0.61304182, 0.86640391,\n",
       "       1.        , 0.77475544, 0.85887897, 1.        , 1.        ,\n",
       "       1.        , 0.68939511, 0.81746945, 0.99997612, 1.        ,\n",
       "       1.        , 0.89117   , 1.        , 0.90864238, 1.        ,\n",
       "       0.98412282, 0.86621193, 1.        , 0.9170185 , 1.        ,\n",
       "       0.50825966, 1.        , 0.59255336, 1.        , 1.        ])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.alphas"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
