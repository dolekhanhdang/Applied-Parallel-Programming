{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "40ygIDXEhjav"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import glob2\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from random import randrange\n",
    "import logging\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='./log.log',level=10, filemode = 'w', force=True, format='%(asctime)s   %(funcName)s - %(levelname)s:%(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXMhyoM3Wnq4"
   },
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_cat = r'./PetImages/Cat/**'\n",
    "link_dog = r'./PetImages/Dog/**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12501\n",
      "12501\n"
     ]
    }
   ],
   "source": [
    "n_samples = 200\n",
    "images  = []\n",
    "labels = []\n",
    "list_cat = glob2.glob(link_cat)\n",
    "print(len(list_cat))\n",
    "for i in range(n_samples):\n",
    "    if('jpg' in list_cat[i]):\n",
    "        img = Image.open(list_cat[i]).convert('RGB')\n",
    "        img = img.resize((400,400), Image.LANCZOS)\n",
    "        if len(np.array(img).shape)  == 3:\n",
    "            images.append(np.array(img))\n",
    "            labels.append(1)\n",
    "            \n",
    "list_dog = glob2.glob(link_dog)\n",
    "print(len(list_dog))\n",
    "for i in range(n_samples):\n",
    "    if('jpg' in list_dog[i]):\n",
    "        img = Image.open(list_dog[i]).convert('RGB')\n",
    "        img = img.resize((400,400), Image.LANCZOS)\n",
    "        if len(np.array(img).shape)  == 3:\n",
    "            images.append(np.array(img))\n",
    "            labels.append(-1)\n",
    "\n",
    "X = np.array(images)\n",
    "y = np.array(labels)\n",
    "\n",
    "for index in range(len(images)):\n",
    "    if images[index].shape[2] != 3:\n",
    "        print(index, images[index].shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oGqEcWO3NQs1"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([x.flatten() for x in X_train])\n",
    "x_test = np.array([x.flatten() for x in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, -1,  1,  1,  1, -1, -1, -1, -1,  1, -1, -1, -1,  1,  1, -1,\n",
       "        1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1,  1,  1, -1, -1, -1,  1,\n",
       "        1, -1, -1,  1,  1,  1, -1, -1,  1, -1,  1, -1,  1, -1, -1,  1, -1,\n",
       "       -1, -1, -1,  1,  1, -1,  1,  1, -1,  1,  1, -1, -1,  1,  1, -1, -1,\n",
       "       -1,  1,  1, -1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1,  1,  1, -1,\n",
       "       -1,  1,  1,  1, -1,  1, -1,  1, -1,  1,  1, -1, -1,  1,  1, -1,  1,\n",
       "       -1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1,  1,  1,  1,  1, -1,\n",
       "        1,  1,  1, -1, -1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1, -1,  1,\n",
       "       -1,  1, -1,  1,  1,  1, -1, -1,  1, -1,  1,  1,  1,  1,  1, -1, -1,\n",
       "       -1, -1,  1, -1, -1,  1, -1,  1,  1, -1, -1, -1, -1, -1,  1, -1, -1,\n",
       "        1, -1,  1, -1,  1,  1, -1, -1,  1, -1, -1,  1,  1,  1,  1, -1,  1,\n",
       "        1, -1, -1,  1, -1,  1, -1, -1,  1, -1, -1, -1,  1, -1,  1,  1,  1,\n",
       "       -1,  1, -1, -1,  1, -1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,\n",
       "       -1, -1,  1,  1, -1, -1,  1,  1,  1,  1,  1, -1,  1, -1,  1, -1,  1,\n",
       "        1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,  1, -1,\n",
       "       -1,  1, -1,  1, -1,  1,  1,  1,  1, -1,  1, -1,  1,  1, -1, -1, -1,\n",
       "       -1, -1,  1, -1,  1,  1, -1, -1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "  def __init__(self, C=1.0, gamma=0.1, tol=1e-3, max_iter=1200):\n",
    "    self.C = C\n",
    "    self.gamma = gamma\n",
    "    self.tol = tol\n",
    "    self.max_iter = max_iter\n",
    "    self.n_iter = 0\n",
    "    self.eps = 1e-5\n",
    "\n",
    "  def rbf(self, x1, x2):\n",
    "    return np.exp(-self.gamma * (np.linalg.norm(x1 - x2) **2 ))\n",
    "  \n",
    "  def output(self, x):\n",
    "    return np.sum([self.alphas[i] * self.y[i] * self.rbf(self.X[i], x) for i in range(self.n_samples)]) - self.b\n",
    "\n",
    "  def get_error(self, i):\n",
    "    return (self.output(self.X[i]) - self.y[i]) if not self.non_bound[i] else self.errors[i]\n",
    "  \n",
    "\n",
    "  def predict(self, X):\n",
    "    pred = []\n",
    "    for x in X:\n",
    "      pred.append(np.sign(self.output(x)))\n",
    "    return pred\n",
    "    \n",
    "  def compute_L_H(self):\n",
    "    if self.y1 != self.y2:\n",
    "      L = max(0, self.alpha2 - self.alpha1)\n",
    "      H = min(self.C, self.C + self.alpha2 - self.alpha1)\n",
    "    else:\n",
    "      L = max(0, self.alpha2 + self.alpha1 - self.C)\n",
    "      H = min(self.C, self.alpha2 + self.alpha1)\n",
    "    return L, H\n",
    "  \n",
    "  def compute_threshold(self, alpha1_new, alpha2_new, k11, k12, k22):\n",
    "    b1 = self.E1 + self.y1 * (alpha1_new - self.alpha1) * k11 + \\\n",
    "        self.y2 * (alpha2_new - self.alpha2) * k12 + self.b\n",
    "    b2 = self.E2 + self.y1 * (alpha1_new - self.alpha1) * k12 + \\\n",
    "        self.y2 * (alpha2_new - self.alpha2) * k22 + self.b\n",
    "\n",
    "    if 0 < alpha1_new and alpha1_new < self.C:\n",
    "      return b1\n",
    "    if 0 < alpha2_new and alpha2_new < self.C:\n",
    "      return b2\n",
    "    return (b1 + b2)/2.0\n",
    "    \n",
    "  def update_error_cache(self, alpha1_new, alpha2_new, i1, i2, old_b):\n",
    "    delta1 = self.y1 * (alpha1_new - self.alpha1)\n",
    "    delta2 = self.y2 * (alpha2_new - self.alpha2)\n",
    "    \n",
    "    for i in self.non_bound_idx():\n",
    "      self.errors[i] += delta1 * self.rbf(self.x1, self.X[i]) + \\\n",
    "                        delta2 * self.rbf(self.x2, self.X[i]) + \\\n",
    "                        old_b - self.b\n",
    "\n",
    "    self.errors[i1] = 0\n",
    "    self.errors[i2] = 0\n",
    "\n",
    "    \n",
    "  def update_non_bound(self, i1, i2):\n",
    "    if 0 < self.alphas[i1] and self.alphas[i1] < self.C:\n",
    "      self.non_bound[i1] = True\n",
    "    else:\n",
    "      self.non_bound[i1] = False\n",
    "      \n",
    "    if 0 < self.alphas[i2] and self.alphas[i2] < self.C:\n",
    "      self.non_bound[i2] = True\n",
    "    else:\n",
    "      self.non_bound[i2] = False\n",
    "\n",
    "  def non_bound_idx(self):\n",
    "    return list(np.array(range(self.n_samples))[self.non_bound])\n",
    "    \n",
    "    \n",
    "  def get_alpha2(self, L, H, k11, k12, k22, s):\n",
    "    eta = k11 + k22 - 2 * k12\n",
    "    if eta > 0:\n",
    "      alpha2_new = self.alpha2 + self.y2 * (self.E1-self.E2)/eta\n",
    "      alpha2_new = min(H, max(L, alpha2_new))\n",
    "    else:\n",
    "      print(\"ETA < 0\")\n",
    "      logging.error(\"ETA <= 0\")\n",
    "      f1 = self.y1*(self.E1 + self.b) - self.alpha1*k11 - s*self.alpha2*k12\n",
    "      f2 = self.y2*(self.E2 + self.b) - s*self.alpha1*k12 - self.alpha2*k22\n",
    "      L1 = self.alpha1 + s*(self.alpha2-L)\n",
    "      H1 = self.alpha1 + s*(self.alpha2-H)\n",
    "      Lobj = L1*f1 + L*f2 + 0.5*(L1**2)*k11 + 0.5*(L**2)*k22 + s*L*L1*k12\n",
    "      Hobj = H1*f1 + H*f2 + 0.5*(H1**2)*k11 + 0.5*(H**2)*k22 + s*H*H1*k12\n",
    "      if Lobj < Hobj - self.eps:\n",
    "        alpha2_new = L\n",
    "      elif Lobj > Hobj + self.eps:\n",
    "        alpha2_new = H\n",
    "      else:\n",
    "        alpha2_new = self.alpha2\n",
    "    return alpha2_new\n",
    "    \n",
    "  def take_step(self, i1, i2):\n",
    "    if i1 == i2:\n",
    "      return 0\n",
    "    self.y2 = self.y[i2]\n",
    "    self.alpha2 = self.alphas[i2]\n",
    "    self.x2 = self.X[i2]\n",
    "    self.E2 = self.get_error(i2)\n",
    "    \n",
    "    s = self.y1 * self.y2\n",
    "    L, H = self.compute_L_H()\n",
    "    if L==H:\n",
    "      return 0\n",
    "    k11 = self.rbf(self.x1, self.x1)\n",
    "    k12 = self.rbf(self.x1, self.x2)\n",
    "    k22 = self.rbf(self.x2, self.x2)\n",
    "    alpha2_new = self.get_alpha2(L, H, k11, k12, k22, s)\n",
    "    if abs(alpha2_new - self.alpha2) < self.eps: # * (alpha2_new + self.alpha2 + self.eps):\n",
    "      return 0\n",
    "    alpha1_new = self.alpha1 + s * (self.alpha2 - alpha2_new)\n",
    "    old_b = self.b\n",
    "    self.b = self.compute_threshold(alpha1_new, alpha2_new, k11, k12, k22)\n",
    "    self.update_error_cache(alpha1_new, alpha2_new, i1, i2, old_b)\n",
    "    self.alphas[i1] = alpha1_new\n",
    "    self.alphas[i2] = alpha2_new\n",
    "    self.update_non_bound(i1, i2)\n",
    "    logging.warning(f\"alpha[{i1}]: {alpha1_new};\\talpha[{i2}]: {alpha2_new}\") #;\\t[{L},  {H}]\")\n",
    "    return 1\n",
    "  \n",
    "  def second_choice_heuristic(self, non_bound_indexes):\n",
    "    i2 = -1\n",
    "    m = 0\n",
    "    for i in non_bound_indexes:\n",
    "      step = abs(self.get_error(i) - self.E1)\n",
    "      if step > m:\n",
    "        m = step\n",
    "        i2 = i\n",
    "    return i2\n",
    "\n",
    "  def examine_example(self, i1):\n",
    "    self.y1 = self.y[i1]\n",
    "    self.alpha1 = self.alphas[i1]\n",
    "    self.x1 = self.X[i1]\n",
    "    self.E1 = self.get_error(i1)\n",
    "    r2 = self.E1 * self.y1\n",
    "    if (r2 < -self.tol and self.alpha1 < self.C) or \\\n",
    "      (r2 > self.tol and self.alpha1 > 0):\n",
    "      non_bound_indexes = self.non_bound_idx()\n",
    "      n_non_bound = len(non_bound_indexes)\n",
    "      logging.info(f\"Found {n_non_bound} non bound\")\n",
    "      if (n_non_bound > 1):\n",
    "        logging.info(\"Branch 1\")\n",
    "        i2 = self.second_choice_heuristic(non_bound_indexes)\n",
    "        if i2 != -1:\n",
    "          logging.info(f\"Take step {i1} and {i2}\")\n",
    "          if self.take_step(i1, i2):\n",
    "            return 1\n",
    "        \n",
    "      if n_non_bound > 0:\n",
    "        logging.info(\"Branch 2\")\n",
    "        rand_i = randrange(n_non_bound)\n",
    "        for i2 in non_bound_indexes[rand_i:] + non_bound_indexes[:rand_i]:\n",
    "          logging.info(f\"Take step {i1} and {i2}\")\n",
    "          if self.take_step(i1, i2):\n",
    "            return 1\n",
    "          \n",
    "      logging.info(\"Branch 3\")\n",
    "      rand_i = randrange(self.n_samples)\n",
    "      all_indexes = list(range(self.n_samples))\n",
    "      for i2 in all_indexes[rand_i:] + all_indexes[:rand_i]:\n",
    "        logging.info(f\"Take step {i1} and {i2}\")\n",
    "        if self.take_step(i1, i2):\n",
    "          return 1\n",
    "    return 0\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    # random.seed(42)\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.n_samples, self.n_features = X.shape\n",
    "    self.errors = np.zeros(self.n_samples)\n",
    "    self.alphas = np.zeros(self.n_samples)\n",
    "    self.non_bound = np.array([False for _ in range(self.n_samples)])\n",
    "    self.b = 0\n",
    "    num_changed = 0\n",
    "    examine_all = True\n",
    "    self.n_iter = 0\n",
    "    while num_changed > 0 or examine_all:\n",
    "      self.n_iter += 1\n",
    "      logging.debug(f\"N iter: {self.n_iter}\")\n",
    "      num_changed = 0\n",
    "      if examine_all:\n",
    "        for i in range(self.n_samples):\n",
    "          num_changed += self.examine_example(i)\n",
    "      else:\n",
    "        for i in range(self.n_samples):\n",
    "          if self.non_bound[i]:\n",
    "            num_changed += self.examine_example(i)\n",
    "  \n",
    "      if examine_all:\n",
    "        examine_all = False\n",
    "      elif num_changed == 0:\n",
    "        examine_all = True\n",
    "      logging.debug(f\"examine_all: {examine_all}, numchanged: {num_changed}\")\n",
    "        \n",
    "    sv_idx = (self.alphas > 0)\n",
    "    logging.info(f\"Filtering support vectors, there are {np.count_nonzero(sv_idx)} alphas\")\n",
    "    self.X = X[sv_idx]\n",
    "    self.y = y[sv_idx]\n",
    "    self.n_samples = np.count_nonzero(sv_idx)\n",
    "    logging.info(\"Done fitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPX7eu3l_--N",
    "outputId": "5c4d5586-0b7f-4f40-b357-58e80195bdfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVM(gamma = 1/(x_train.shape[1]*x_train.var()))\n",
    "svm.fit(x_train, y_train)\n",
    "pred = svm.predict(x_test)\n",
    "accuracy_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86481786, 0.97137444, 1.        , 0.77498286, 0.80565165,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.97849338,\n",
       "       1.        , 1.        , 0.97012512, 0.82428997, 0.73560782,\n",
       "       0.82331692, 1.        , 1.        , 0.92556372, 0.79433743,\n",
       "       1.        , 1.        , 0.79817157, 0.97629247, 1.        ,\n",
       "       0.83890892, 1.        , 0.83220308, 1.        , 0.4248626 ,\n",
       "       1.        , 1.        , 1.        , 0.7399969 , 0.84531939,\n",
       "       0.92245986, 1.        , 1.        , 1.        , 0.97738655,\n",
       "       1.        , 0.72425094, 0.85073714, 0.70486596, 0.97538143,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.51815235, 1.        , 0.96615093, 0.89360544,\n",
       "       0.73773213, 1.        , 0.91439841, 1.        , 0.75398319,\n",
       "       0.94790906, 1.        , 1.        , 0.8883202 , 0.90451512,\n",
       "       0.83790367, 0.88799694, 1.        , 1.        , 0.89469757,\n",
       "       0.93821919, 0.88863783, 1.        , 1.        , 0.91075086,\n",
       "       0.83242418, 1.        , 0.45744848, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.91530128, 0.9629647 ,\n",
       "       1.        , 0.93161178, 1.        , 0.69477603, 0.89415544,\n",
       "       1.        , 0.94280262, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.85608394, 1.        , 1.        , 0.72999284, 1.        ,\n",
       "       0.99349835, 1.        , 0.80673314, 0.9974221 , 0.70224859,\n",
       "       0.94070102, 1.        , 1.        , 1.        , 0.95831749,\n",
       "       0.89761843, 0.22631791, 1.        , 0.65210672, 1.        ,\n",
       "       0.92594513, 0.83367512, 0.61718333, 0.94272089, 0.78718716,\n",
       "       0.96295291, 0.91797766, 0.70222356, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.92325582, 1.        , 0.62552147,\n",
       "       0.8421098 , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.83864302, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.8876775 , 1.        , 0.59112102, 1.        ,\n",
       "       1.        , 1.        , 0.78219754, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.97467586, 0.9112768 , 1.        ,\n",
       "       0.99088344, 1.        , 0.9258693 , 0.97904917, 0.98267541,\n",
       "       0.5443416 , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.93213316, 0.8598636 , 1.        , 1.        , 1.        ,\n",
       "       0.95188819, 0.80701882, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.78923466, 1.        , 1.        , 0.92894812,\n",
       "       0.67331745, 0.96255559, 1.        , 0.67802807, 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.83958432, 1.        ,\n",
       "       0.93437994, 0.95708705, 0.8819181 , 1.        , 0.82784966,\n",
       "       0.61654321, 0.92045803, 0.87165071, 0.84598583, 1.        ,\n",
       "       0.83289582, 0.94019582, 1.        , 1.        , 1.        ,\n",
       "       0.96611058, 0.81392717, 0.80323035, 0.        , 0.77783552,\n",
       "       0.73174399, 1.        , 0.94606891, 0.86940896, 0.92298518,\n",
       "       0.88125151, 0.98249059, 0.84639619, 1.        , 0.90445244,\n",
       "       0.87050889, 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.96926933, 0.90454958, 0.98707205, 1.        , 1.        ,\n",
       "       1.        , 0.88494393, 0.98888161, 0.9191687 , 1.        ,\n",
       "       0.94330737, 0.82804678, 0.98180013, 0.96848725, 0.53773847,\n",
       "       0.82031958, 1.        , 0.9621731 , 1.        , 0.84906498,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.79579633,\n",
       "       0.74674751, 1.        , 1.        , 1.        , 0.95080747,\n",
       "       1.        , 0.93863125, 0.94136669, 0.7327107 , 0.9902245 ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.91375404,\n",
       "       0.99881716, 1.        , 0.59513167, 1.        , 0.86217968,\n",
       "       0.91842792, 1.        , 0.94680749, 0.82242496, 1.        ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpmSnc60Aa8F",
    "outputId": "9f9be904-2184-40d2-9baf-a4b84dcb399b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6416666666666667"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_sklearn = np.array([str(y) for y in y_train])\n",
    "y_test_sklearn = np.array([str(y) for y in y_test])\n",
    "\n",
    "svc = SVC(kernel='rbf', gamma = 1/(x_train.shape[1]*x_train.var()))\n",
    "svc.fit(x_train, y_train_sklearn)\n",
    "pred = svc.predict(x_test)\n",
    "accuracy_score(pred, y_test_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        , -0.2193241 , -1.        , -1.        , -0.00149065,\n",
       "        -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "        -0.74376967, -1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -0.70302959, -0.99574444, -1.        , -1.        ,\n",
       "        -1.        , -1.        , -0.23701492, -0.76843078, -1.        ,\n",
       "        -1.        , -0.30496195, -1.        , -1.        , -1.        ,\n",
       "        -0.74114282, -1.        , -1.        , -0.82815411, -1.        ,\n",
       "        -0.47173847, -1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        , -1.        , -0.74809962,\n",
       "        -1.        , -1.        , -0.25516863, -1.        , -1.        ,\n",
       "        -1.        , -1.        , -0.61541094, -0.25890613, -1.        ,\n",
       "        -1.        , -0.80668513, -1.        , -0.7056271 , -1.        ,\n",
       "        -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "        -0.57047762, -0.8331004 , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        , -0.30601113, -1.        ,\n",
       "        -0.83594669, -1.        , -1.        , -1.        , -0.39180418,\n",
       "        -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -0.87704307, -0.80019163, -1.        , -0.88226644,\n",
       "        -0.98383228, -1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -0.19893164, -1.        , -1.        , -1.        ,\n",
       "        -1.        , -1.        , -0.22278321, -1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        , -1.        , -0.61559162,\n",
       "        -0.90997908, -1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -0.6235974 , -0.73611061, -1.        , -0.64664988,\n",
       "        -1.        , -1.        , -1.        , -1.        , -0.82921514,\n",
       "        -0.39828935, -0.99116529,  0.80722203,  0.27359305,  0.51387015,\n",
       "         1.        ,  1.        ,  0.333148  ,  1.        ,  0.93179986,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.48611375,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.05201729,  0.94213368,  1.        ,  1.        ,  0.87295814,\n",
       "         0.47393079,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.30041134,  1.        ,\n",
       "         1.        ,  0.20533279,  0.88855344,  1.        ,  0.8124838 ,\n",
       "         0.65394561,  0.56772826,  1.        ,  1.        ,  1.        ,\n",
       "         0.86847535,  0.79534254,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.95154927,  1.        ,\n",
       "         1.        ,  1.        ,  0.80639025,  0.77746784,  1.        ,\n",
       "         0.97582095,  0.62780997,  1.        ,  0.87768018,  0.815092  ,\n",
       "         0.7325027 ,  1.        ,  1.        ,  0.43276976,  0.64610382,\n",
       "         0.8943593 ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.85046435,  1.        ,  1.        ,  0.80964927,\n",
       "         0.8744544 ,  1.        ,  0.73211035,  0.97818435,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.53324154,\n",
       "         1.        ,  1.        ,  0.22594894,  1.        ,  0.97078478,\n",
       "         0.34671665,  1.        ,  1.        ,  0.78487798,  0.69581115,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.05072626,  1.        ,  1.        ,  0.15727235,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.51916112,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.71553295,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.45420577,  1.        ,\n",
       "         0.58720032,  0.45473729]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.dual_coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_simplified:\n",
    "    def __init__(self, C=1.0, gamma=0.1, tol=1e-3, max_iter=1200):\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.alphas = None\n",
    "        self.b = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.n_iter_ = 0\n",
    "\n",
    "    def rbf_kernel(self, x1, x2):\n",
    "      return np.exp(-self.gamma * (np.linalg.norm(x1 - x2) ** 2))\n",
    "    \n",
    "    def compute_error(self, n_samples, y, i, K):\n",
    "      err = self.b - y[i]\n",
    "      for j in range(n_samples):\n",
    "        err += self.alphas[j] * y[j] * K[j, i]\n",
    "      return err\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "      # random.seed(42)\n",
    "      n_samples, n_features = X.shape\n",
    "\n",
    "      K = np.zeros((n_samples, n_samples))\n",
    "      for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "          K[i,j] = self.rbf_kernel(X[i], X[j])\n",
    "      \n",
    "      self.alphas = np.zeros(n_samples)\n",
    "      self.b = 0\n",
    "      passes = 0\n",
    "      while passes < self.max_iter:\n",
    "        self.n_iter_ += 1\n",
    "        num_changed_alphas = 0\n",
    "        for i in range(n_samples):\n",
    "          Ei = np.sum(self.alphas*y*K[i,:]) + self.b - y[i]\n",
    "          # Ei = self.compute_error(n_samples, y, i, K)\n",
    "          # Ei = np.sum(np.dot(np.dot(self.alphas, y), K[i,:])) +self.b - y[i]\n",
    "          # if abs(ei - Ei)>0.00001:\n",
    "          #   print(Ei, ei)\n",
    "          if (((y[i]*Ei < -self.tol) and (self.alphas[i] < self.C)) or ((y[i]*Ei>self.tol) and (self.alphas[i]>0))):\n",
    "            j = random.choice(list(range(i))+list(range(i+1,n_samples)))\n",
    "            Ej = np.sum(self.alphas*y*K[j, :]) + self.b - y[j]\n",
    "            # Ej = self.compute_error(n_samples, y, j, K)\n",
    "            # Ej = np.sum(np.dot(np.dot(self.alphas, y), K[j,:])) +self.b - y[j]\n",
    "            alpha_i_old = self.alphas[i]\n",
    "            alpha_j_old = self.alphas[j]\n",
    "            if (y[i]!=y[j]):\n",
    "              L = max(0, self.alphas[j] - self.alphas[i])\n",
    "              H = min(self.C, self.C + self.alphas[j] - self.alphas[i])\n",
    "            else:\n",
    "              L = max(0, self.alphas[i] + self.alphas[j] - self.C)\n",
    "              H = min(self.C, self.alphas[i] + self.alphas[j])\n",
    "            if L==H:\n",
    "              continue\n",
    "            eta = 2 * K[i,j] - K[i,i] - K[j,j]\n",
    "            if eta>=0:\n",
    "              continue\n",
    "            self.alphas[j] -= y[j]*(Ei-Ej)/eta\n",
    "            self.alphas[j] = max(L, min(H, self.alphas[j]))\n",
    "\n",
    "            if (abs(self.alphas[j] - alpha_j_old) < 0.00001):\n",
    "              continue\n",
    "            self.alphas[i] += y[i]*y[j]*(alpha_j_old - self.alphas[j])\n",
    "            \n",
    "            b1 = self.b - Ei - y[i]*(self.alphas[i]-alpha_i_old)*K[i,i] - y[j]*(self.alphas[j]-alpha_j_old)*K[i,j]\n",
    "            b2 = self.b - Ej - y[i]*(self.alphas[i]-alpha_i_old)*K[i,j] - y[j]*(self.alphas[j]-alpha_j_old)*K[j,j]\n",
    "            if (0 < self.alphas[i] and self.alphas[i]<self.C):\n",
    "              self.b = b1\n",
    "            elif (0<self.alphas[j] and self.alphas[j]<self.C):\n",
    "              self.b = b2\n",
    "            else:\n",
    "              self.b = (b1 + b2)/2\n",
    "            num_changed_alphas += 1\n",
    "        if (num_changed_alphas == 0):\n",
    "          passes+= 1\n",
    "        else:\n",
    "          passes = 0\n",
    "      sv_idx = (self.alphas>0)\n",
    "      self.sv_idx = sv_idx\n",
    "      self.alphas = self.alphas[sv_idx]\n",
    "      self.support_vectors = X[sv_idx]\n",
    "      self.support_vector_labels = y[sv_idx]\n",
    "    def predict(self, X):\n",
    "      y_pred = np.zeros(len(X))\n",
    "      for i in range(len(X)):\n",
    "        s = self.b\n",
    "        for alpha, sv_y, sv_x in zip(self.alphas, self.support_vector_labels, self.support_vectors):\n",
    "          s += alpha * sv_y * self.rbf_kernel(X[i], sv_x)\n",
    "        y_pred[i] = s\n",
    "      return np.sign(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVM_simplified(gamma = 1/(x_train.shape[1]*x_train.var()))\n",
    "svm.fit(x_train, y_train)\n",
    "pred = svm.predict(x_test)\n",
    "accuracy_score(pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
