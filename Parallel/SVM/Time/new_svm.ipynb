{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "40ygIDXEhjav"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import glob2\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from random import randrange\n",
    "import logging\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='./log.log',level=10, filemode = 'w', force=True, format='%(asctime)s   %(funcName)s - %(levelname)s:%(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXMhyoM3Wnq4"
   },
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_cat = r'C:\\Users\\dolek\\Documents\\GitHub\\Applied-Parallel-Programming\\Parallel\\SVM\\PetImages\\Cat\\**'\n",
    "link_dog = r'C:\\Users\\dolek\\Documents\\GitHub\\Applied-Parallel-Programming\\Parallel\\SVM\\PetImages\\Dog\\**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12501\n",
      "12501\n"
     ]
    }
   ],
   "source": [
    "n_samples = 200\n",
    "images  = []\n",
    "labels = []\n",
    "list_cat = glob2.glob(link_cat)\n",
    "print(len(list_cat))\n",
    "for i in range(n_samples):\n",
    "    if('jpg' in list_cat[i]):\n",
    "        img = Image.open(list_cat[i]).convert('RGB')\n",
    "        img = img.resize((400,400), Image.LANCZOS)\n",
    "        if len(np.array(img).shape)  == 3:\n",
    "            images.append(np.array(img))\n",
    "            labels.append(1)\n",
    "            \n",
    "list_dog = glob2.glob(link_dog)\n",
    "print(len(list_dog))\n",
    "for i in range(n_samples):\n",
    "    if('jpg' in list_dog[i]):\n",
    "        img = Image.open(list_dog[i]).convert('RGB')\n",
    "        img = img.resize((400,400), Image.LANCZOS)\n",
    "        if len(np.array(img).shape)  == 3:\n",
    "            images.append(np.array(img))\n",
    "            labels.append(-1)\n",
    "\n",
    "X = np.array(images)\n",
    "y = np.array(labels)\n",
    "\n",
    "for index in range(len(images)):\n",
    "    if images[index].shape[2] != 3:\n",
    "        print(index, images[index].shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oGqEcWO3NQs1"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([x.flatten() for x in X_train])\n",
    "x_test = np.array([x.flatten() for x in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1,  1, -1, -1, -1, -1,  1,  1, -1,  1, -1, -1,  1,  1, -1,\n",
       "       -1, -1, -1, -1,  1,  1,  1, -1, -1,  1,  1,  1,  1, -1, -1,  1,  1,\n",
       "       -1,  1,  1,  1,  1,  1, -1,  1,  1,  1, -1, -1, -1,  1, -1,  1, -1,\n",
       "        1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1, -1, -1, -1,  1, -1,\n",
       "        1,  1, -1, -1,  1,  1, -1, -1, -1,  1,  1, -1, -1,  1, -1,  1, -1,\n",
       "        1, -1, -1,  1, -1, -1,  1,  1, -1,  1, -1,  1, -1,  1,  1,  1, -1,\n",
       "       -1,  1, -1,  1,  1, -1, -1, -1,  1, -1, -1, -1, -1,  1,  1, -1,  1,\n",
       "        1, -1, -1, -1,  1,  1, -1,  1, -1, -1, -1, -1,  1, -1,  1, -1,  1,\n",
       "        1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1,  1,\n",
       "        1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1, -1,  1, -1,\n",
       "        1, -1,  1,  1,  1, -1,  1, -1, -1, -1, -1,  1, -1, -1,  1, -1, -1,\n",
       "       -1, -1,  1, -1, -1,  1, -1, -1, -1,  1,  1,  1,  1,  1, -1, -1, -1,\n",
       "       -1, -1,  1, -1, -1,  1,  1, -1, -1,  1,  1, -1,  1,  1,  1,  1, -1,\n",
       "        1,  1, -1, -1, -1,  1, -1,  1, -1,  1,  1,  1, -1, -1, -1, -1,  1,\n",
       "        1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1, -1, -1, -1,  1,  1, -1,\n",
       "       -1, -1,  1,  1,  1, -1, -1, -1, -1,  1, -1, -1, -1,  1,  1,  1,  1,\n",
       "        1,  1, -1, -1, -1,  1, -1, -1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "  def __init__(self, C=1.0, gamma=0.1, tol=1e-3, max_iter=1200):\n",
    "    self.C = C\n",
    "    self.gamma = gamma\n",
    "    self.tol = tol\n",
    "    self.max_iter = max_iter\n",
    "    self.n_iter = 0\n",
    "    self.eps = 1e-5\n",
    "\n",
    "  def rbf(self, x1, x2):\n",
    "    return np.exp(-self.gamma * (np.linalg.norm(x1 - x2) **2 ))\n",
    "  \n",
    "  def output(self, x):\n",
    "    return np.sum([self.alphas[i] * self.y[i] * self.rbf(self.X[i], x) for i in range(self.n_samples)]) - self.b\n",
    "\n",
    "  def get_error(self, i):\n",
    "    if self.non_bound[i]:\n",
    "      return self.errors[i]\n",
    "    else:\n",
    "      op = self.output(self.X[i]) - self.y[i]\n",
    "      self.errors[i] = op\n",
    "      return op  \n",
    "\n",
    "  def predict(self, X):\n",
    "    pred = []\n",
    "    for x in X:\n",
    "      pred.append(np.sign(self.output(x)))\n",
    "    return pred\n",
    "    \n",
    "  def compute_L_H(self):\n",
    "    if self.y1 != self.y2:\n",
    "      L = max(0, self.alpha2 - self.alpha1)\n",
    "      H = min(self.C, self.C + self.alpha2 - self.alpha1)\n",
    "    else:\n",
    "      L = max(0, self.alpha2 + self.alpha1 - self.C)\n",
    "      H = min(self.C, self.alpha2 + self.alpha1)\n",
    "    return L, H\n",
    "  \n",
    "  def compute_threshold(self, alpha1_new, alpha2_new, k11, k12, k22):\n",
    "    b1 = self.E1 + self.y1 * (alpha1_new - self.alpha1) * k11 + \\\n",
    "        self.y2 * (alpha2_new - self.alpha2) * k12 + self.b\n",
    "    b2 = self.E2 + self.y1 * (alpha1_new - self.alpha1) * k12 + \\\n",
    "        self.y2 * (alpha2_new - self.alpha2) * k22 + self.b\n",
    "\n",
    "    if 0 < alpha1_new and alpha1_new < self.C:\n",
    "      return b1\n",
    "    if 0 < alpha2_new and alpha2_new < self.C:\n",
    "      return b2\n",
    "    return (b1 + b2)/2.0\n",
    "    \n",
    "  def update_error_cache(self, alpha1_new, alpha2_new, i1, i2, old_b):\n",
    "    delta1 = self.y1 * (alpha1_new - self.alpha1)\n",
    "    delta2 = self.y2 * (alpha2_new - self.alpha2)\n",
    "    # old1 = self.errors[i1]\n",
    "    # old2 = self.errors[i2]\n",
    "    # non_bound_indexes = self.non_bound_idx()\n",
    "    # logging.info(non_bound_indexes)\n",
    "    for i in self.non_bound_idx:\n",
    "      self.errors[i] += delta1 * self.rbf(self.x1, self.X[i]) + \\\n",
    "                        delta2 * self.rbf(self.x2, self.X[i]) + \\\n",
    "                        old_b - self.b\n",
    "    # new1 = self.errors[i1]\n",
    "    # new2 = self.errors[i2]\n",
    "    # true1 = self.output(self.x1) - self.y1\n",
    "    # true2 = self.output(self.x2) - self.y2\n",
    "    # logging.info(f\"{i1}: old:{old1},\\tnew:{new1},\\ttrue:{true1}\")\n",
    "    # logging.info(f\"{i2}: old:{old2},\\tnew:{new2},\\ttrue:{true2}\")\n",
    "\n",
    "    \n",
    "  def update_non_bound(self, i1, i2):\n",
    "    if 0 < self.alphas[i1] and self.alphas[i1] < self.C:\n",
    "      self.non_bound[i1] = True\n",
    "      if i1 not in self.non_bound_idx:\n",
    "        self.non_bound_idx.append(i1)\n",
    "    else:\n",
    "      self.non_bound[i1] = False\n",
    "      if i1 in self.non_bound_idx:\n",
    "        self.non_bound_idx.remove(i1)\n",
    "      \n",
    "    if 0 < self.alphas[i2] and self.alphas[i2] < self.C:\n",
    "      self.non_bound[i2] = True\n",
    "      if i2 not in self.non_bound_idx:\n",
    "        self.non_bound_idx.append(i2)\n",
    "    else:\n",
    "      self.non_bound[i2] = False\n",
    "      if i2 in self.non_bound_idx:\n",
    "        self.non_bound_idx.remove(i2)\n",
    "\n",
    "  # def non_bound_idx(self):\n",
    "  #   return list(np.array(range(self.n_samples))[self.non_bound])\n",
    "    \n",
    "    \n",
    "  def get_alpha2(self, L, H, k11, k12, k22, s):\n",
    "    eta = k11 + k22 - 2 * k12\n",
    "    if eta > 0:\n",
    "      alpha2_new = self.alpha2 + self.y2 * (self.E1-self.E2)/eta\n",
    "      alpha2_new = min(H, max(L, alpha2_new))\n",
    "    else:\n",
    "      print(\"ETA < 0\")\n",
    "      logging.error(\"ETA <= 0\")\n",
    "      f1 = self.y1*(self.E1 + self.b) - self.alpha1*k11 - s*self.alpha2*k12\n",
    "      f2 = self.y2*(self.E2 + self.b) - s*self.alpha1*k12 - self.alpha2*k22\n",
    "      L1 = self.alpha1 + s*(self.alpha2-L)\n",
    "      H1 = self.alpha1 + s*(self.alpha2-H)\n",
    "      Lobj = L1*f1 + L*f2 + 0.5*(L1**2)*k11 + 0.5*(L**2)*k22 + s*L*L1*k12\n",
    "      Hobj = H1*f1 + H*f2 + 0.5*(H1**2)*k11 + 0.5*(H**2)*k22 + s*H*H1*k12\n",
    "      if Lobj < Hobj - self.eps:\n",
    "        alpha2_new = L\n",
    "      elif Lobj > Hobj + self.eps:\n",
    "        alpha2_new = H\n",
    "      else:\n",
    "        alpha2_new = self.alpha2\n",
    "    return alpha2_new\n",
    "    \n",
    "  def take_step(self, i1, i2):\n",
    "    if i1 == i2:\n",
    "      return 0\n",
    "    self.y2 = self.y[i2]\n",
    "    self.alpha2 = self.alphas[i2]\n",
    "    self.x2 = self.X[i2]\n",
    "    self.E2 = self.get_error(i2)\n",
    "    # e2 = self.output(self.x2) - self.y2\n",
    "    # if abs(e2 - self.E2) > 0.00001:\n",
    "    #   logging.info(f\"E[{i2}]: {abs(e2-self.E2)}\")\n",
    "    # self.E2 = e2\n",
    "    s = self.y1 * self.y2\n",
    "    L, H = self.compute_L_H()\n",
    "    if L==H:\n",
    "      return 0\n",
    "    k11 = self.rbf(self.x1, self.x1)\n",
    "    k12 = self.rbf(self.x1, self.x2)\n",
    "    k22 = self.rbf(self.x2, self.x2)\n",
    "    self.alphas[i2] = self.get_alpha2(L, H, k11, k12, k22, s)\n",
    "    if abs(self.alphas[i2] - self.alpha2) < self.eps: # * (alpha2_new + self.alpha2 + self.eps):\n",
    "      return 0\n",
    "    self.alphas[i1] = self.alpha1 + s * (self.alpha2 - self.alphas[i2])\n",
    "    old_b = self.b\n",
    "    self.b = self.compute_threshold(self.alphas[i1], self.alphas[i2], k11, k12, k22)\n",
    "    self.update_non_bound(i1, i2)\n",
    "    self.update_error_cache(self.alphas[i1], self.alphas[i2], i1, i2, old_b)\n",
    "    # self.alphas[i1] = self.alphas[i1]\n",
    "    # self.alphas[i2] = self.alphas[i2]\n",
    "    logging.warning(f\"alpha[{i1}]: {self.alphas[i1]};\\talpha[{i2}]: {self.alphas[i2]}\") #;\\t[{L},  {H}]\")\n",
    "    return 1\n",
    "  \n",
    "  def second_choice_heuristic(self):\n",
    "    i2 = -1\n",
    "    m = 0\n",
    "    for i in self.non_bound_idx:\n",
    "      step = abs(self.get_error(i) - self.E1)\n",
    "      if step > m:\n",
    "        m = step\n",
    "        i2 = i\n",
    "    return i2\n",
    "\n",
    "  def examine_example(self, i1):\n",
    "    self.y1 = self.y[i1]\n",
    "    self.alpha1 = self.alphas[i1]\n",
    "    self.x1 = self.X[i1]\n",
    "    self.E1 = self.get_error(i1)\n",
    "    # e1 = self.output(self.x1) - self.y1\n",
    "    # if abs(e1 - self.E1) > 0.00001:\n",
    "    #   logging.info(f\"E[{i1}]: {abs(e1-self.E1)}\")\n",
    "    # self.E1 = e1\n",
    "    r2 = self.E1 * self.y1\n",
    "    if (r2 < -self.tol and self.alpha1 < self.C) or \\\n",
    "      (r2 > self.tol and self.alpha1 > 0):\n",
    "      # non_bound_indexes = self.non_bound_idx()\n",
    "      n_non_bound = len(self.non_bound_idx)\n",
    "      # n_non_bound = len(non_bound_indexes)\n",
    "      # logging.info(f\"Found {n_non_bound} non bound\")\n",
    "      if (n_non_bound > 1):\n",
    "        # logging.info(\"Branch 1\")\n",
    "        i2 = self.second_choice_heuristic()\n",
    "        if i2 != -1:\n",
    "          # logging.info(f\"Take step {i1} and {i2}\")\n",
    "          if self.take_step(i1, i2):\n",
    "            return 1\n",
    "        \n",
    "      if n_non_bound > 0:\n",
    "        # logging.info(\"Branch 2\")\n",
    "        rand_i = randrange(n_non_bound)\n",
    "        for i2 in self.non_bound_idx[rand_i:] + self.non_bound_idx[:rand_i]:\n",
    "          # logging.info(f\"Take step {i1} and {i2}\")\n",
    "          if self.take_step(i1, i2):\n",
    "            return 1\n",
    "          \n",
    "      # logging.info(\"Branch 3\")\n",
    "      rand_i = randrange(self.n_samples)\n",
    "      all_indexes = list(range(self.n_samples))\n",
    "      for i2 in all_indexes[rand_i:] + all_indexes[:rand_i]:\n",
    "        # logging.info(f\"Take step {i1} and {i2}\")\n",
    "        if self.take_step(i1, i2):\n",
    "          return 1\n",
    "    return 0\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    # random.seed(42)\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.n_samples, self.n_features = X.shape\n",
    "    self.errors = np.zeros(self.n_samples)\n",
    "    self.alphas = np.zeros(self.n_samples)\n",
    "    self.non_bound = np.array([False for _ in range(self.n_samples)])\n",
    "    self.non_bound_idx = []\n",
    "    self.b = 0\n",
    "    num_changed = 0\n",
    "    examine_all = True\n",
    "    self.n_iter = 0\n",
    "    while num_changed > 0 or examine_all:\n",
    "      self.n_iter += 1\n",
    "      logging.debug(f\"N iter: {self.n_iter}\")\n",
    "      logging.debug(f\"examine_all: {examine_all}, numchanged: {num_changed}\")\n",
    "      num_changed = 0\n",
    "      if examine_all:\n",
    "        for i in range(self.n_samples):\n",
    "          num_changed += self.examine_example(i)\n",
    "      else:\n",
    "        for i in range(self.n_samples):\n",
    "          if 0 < self.alphas[i] < self.C:\n",
    "            num_changed += self.examine_example(i)\n",
    "  \n",
    "      if examine_all:\n",
    "        examine_all = False\n",
    "      elif num_changed == 0:\n",
    "        examine_all = True\n",
    "        \n",
    "    sv_idx = (self.alphas > 0)\n",
    "    logging.info(f\"Filtering support vectors, there are {np.count_nonzero(sv_idx)} alphas\")\n",
    "    self.X = X[sv_idx]\n",
    "    self.y = y[sv_idx]\n",
    "    self.n_samples = np.count_nonzero(sv_idx)\n",
    "    logging.info(\"Done fitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPX7eu3l_--N",
    "outputId": "5c4d5586-0b7f-4f40-b357-58e80195bdfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5083333333333333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVM(gamma = 1/(x_train.shape[1]*x_train.var()))\n",
    "svm.fit(x_train, y_train)\n",
    "pred = svm.predict(x_test)\n",
    "accuracy_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.82675409, 0.9872357 , 0.99072339, 0.94468494,\n",
       "       0.61080661, 1.        , 0.37959255, 1.        , 1.        ,\n",
       "       1.        , 0.9848002 , 0.95461195, 0.47748769, 0.88050653,\n",
       "       0.95283551, 1.        , 0.67737543, 1.        , 1.        ,\n",
       "       0.76147385, 1.        , 0.97070077, 1.        , 0.51132466,\n",
       "       0.68547792, 0.9639339 , 0.93580805, 0.95460578, 1.        ,\n",
       "       0.69776102, 0.69455861, 0.90424509, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.8615214 , 1.        , 0.82666023,\n",
       "       1.        , 1.        , 0.72767388, 1.        , 0.88152859,\n",
       "       0.98478475, 1.        , 0.83432565, 0.80405516, 0.78113431,\n",
       "       1.        , 1.        , 0.83890582, 0.95254332, 1.        ,\n",
       "       1.        , 0.86586679, 1.        , 0.85221835, 1.        ,\n",
       "       0.92098516, 0.97934556, 1.        , 1.        , 0.78883948,\n",
       "       0.88290034, 1.        , 0.8640371 , 1.        , 1.        ,\n",
       "       1.        , 0.84649426, 1.        , 0.88947698, 1.        ,\n",
       "       0.91703034, 0.9313868 , 1.        , 0.87650594, 1.        ,\n",
       "       1.        , 0.97588203, 1.        , 1.        , 0.91803705,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.94648747,\n",
       "       0.9397184 , 0.76656099, 0.99611807, 1.        , 1.        ,\n",
       "       0.88493944, 0.76444258, 0.84724656, 1.        , 0.90652003,\n",
       "       1.        , 1.        , 0.91209246, 1.        , 0.90210231,\n",
       "       1.        , 0.87597053, 1.        , 1.        , 0.90088713,\n",
       "       0.86212474, 1.        , 1.        , 0.92243553, 1.        ,\n",
       "       0.95705447, 0.83008717, 0.71004218, 0.91960667, 1.        ,\n",
       "       1.        , 0.79007168, 0.51981409, 0.92036728, 1.        ,\n",
       "       0.39273737, 0.98302631, 1.        , 0.75493471, 1.        ,\n",
       "       1.        , 0.79279503, 0.4641702 , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.92350711, 0.53956864, 1.        , 0.74236174, 0.83834437,\n",
       "       1.        , 1.        , 0.87467691, 1.        , 1.        ,\n",
       "       0.8685807 , 1.        , 0.89103783, 0.87009719, 1.        ,\n",
       "       1.        , 0.70952213, 0.74958495, 1.        , 0.71831167,\n",
       "       1.        , 0.61621161, 1.        , 0.86020259, 0.95234024,\n",
       "       1.        , 0.76126072, 0.9813192 , 1.        , 1.        ,\n",
       "       0.85556517, 1.        , 1.        , 0.33584591, 1.        ,\n",
       "       0.76937308, 1.        , 0.6609849 , 0.9482643 , 0.9208429 ,\n",
       "       1.        , 1.        , 0.8689762 , 0.30225228, 1.        ,\n",
       "       1.        , 0.92258255, 1.        , 0.8003638 , 0.7903015 ,\n",
       "       0.86093692, 1.        , 1.        , 0.9348426 , 0.98556926,\n",
       "       0.64746852, 0.73391683, 0.88351301, 0.97327859, 1.        ,\n",
       "       0.80483463, 1.        , 0.76275737, 1.        , 1.        ,\n",
       "       0.68611031, 0.9196104 , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.95892057, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.92232402, 0.37892128, 0.92725123, 1.        ,\n",
       "       0.76227301, 1.        , 0.82217391, 0.93875571, 0.91078863,\n",
       "       0.76256936, 1.        , 1.        , 0.93423674, 0.99425234,\n",
       "       0.94251836, 0.81642144, 0.94320158, 0.88844693, 1.        ,\n",
       "       1.        , 0.95454237, 0.8184796 , 0.        , 1.        ,\n",
       "       1.        , 0.99361762, 0.56226197, 1.        , 1.        ,\n",
       "       1.        , 0.81866764, 0.98578296, 1.        , 1.        ,\n",
       "       0.88476222, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.85718401, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.82473404, 1.        , 0.77826135, 0.97973879,\n",
       "       1.        , 1.        , 0.93702744, 1.        , 0.9928042 ,\n",
       "       0.88866118, 0.87500519, 1.        , 1.        , 0.74529812,\n",
       "       1.        , 0.47484689, 1.        , 0.84754539, 1.        ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpmSnc60Aa8F",
    "outputId": "9f9be904-2184-40d2-9baf-a4b84dcb399b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6333333333333333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_sklearn = np.array([str(y) for y in y_train])\n",
    "y_test_sklearn = np.array([str(y) for y in y_test])\n",
    "\n",
    "svc = SVC(kernel='rbf', gamma = 1/(x_train.shape[1]*x_train.var()))\n",
    "svc.fit(x_train, y_train_sklearn)\n",
    "pred = svc.predict(x_test)\n",
    "accuracy_score(pred, y_test_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        , -0.27460889, -1.        , -1.        , -1.        ,\n",
       "        -1.        , -0.26592259, -0.8719422 , -0.95740711, -0.9698731 ,\n",
       "        -0.54915002, -0.81658688, -1.        , -1.        , -0.93360672,\n",
       "        -1.        , -0.96197222, -1.        , -1.        , -1.        ,\n",
       "        -1.        , -0.72094847, -1.        , -1.        , -1.        ,\n",
       "        -0.67133595, -1.        , -1.        , -1.        , -0.75695069,\n",
       "        -0.39630082, -0.35685076, -0.87943472, -0.93903037, -0.88491723,\n",
       "        -1.        , -1.        , -0.9593152 , -1.        , -0.99062341,\n",
       "        -1.        , -0.80127303, -1.        , -0.78124839, -0.24693528,\n",
       "        -0.68390454, -1.        , -1.        , -0.05486863, -1.        ,\n",
       "        -1.        , -1.        , -1.        , -1.        , -0.69713346,\n",
       "        -1.        , -0.67051339, -1.        , -0.71516017, -0.585297  ,\n",
       "        -1.        , -1.        , -0.62371337, -1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -0.07080324, -0.8007772 , -1.        , -1.        ,\n",
       "        -0.58048874, -1.        , -1.        , -1.        , -1.        ,\n",
       "        -0.85402772, -1.        , -1.        , -0.22627539, -1.        ,\n",
       "        -0.45094618, -0.78746471, -1.        , -1.        , -0.94715561,\n",
       "        -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "        -0.97640559, -1.        , -0.4422148 , -1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        , -1.        , -0.85459164,\n",
       "        -0.48655418, -1.        , -1.        , -1.        , -1.        ,\n",
       "        -0.69545766, -1.        , -1.        , -0.93717447, -1.        ,\n",
       "        -0.61271236, -0.63241691, -1.        , -0.49620039, -1.        ,\n",
       "        -1.        , -1.        , -0.98618344, -1.        , -1.        ,\n",
       "        -0.2471899 , -0.78593809, -0.66366466, -1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        , -0.79698234, -1.        ,\n",
       "        -1.        , -1.        , -1.        , -0.48208407, -1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.86459726,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.76588914,\n",
       "         1.        ,  1.        ,  0.82443077,  1.        ,  0.98253395,\n",
       "         0.31797765,  0.787901  ,  1.        ,  1.        ,  0.56173824,\n",
       "         0.75005626,  1.        ,  0.88549923,  1.        ,  1.        ,\n",
       "         1.        ,  0.91884255,  1.        ,  0.63290693,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.76782385,\n",
       "         1.        ,  0.85072612,  1.        ,  0.97021038,  1.        ,\n",
       "         1.        ,  1.        ,  0.67478596,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.34526001,  0.74488039,\n",
       "         1.        ,  1.        ,  0.22649956,  1.        ,  0.86335186,\n",
       "         0.93067252,  1.        ,  1.        ,  1.        ,  0.82306594,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.69205357,\n",
       "         1.        ,  0.70580446,  1.        ,  1.        ,  0.7765229 ,\n",
       "         1.        ,  1.        ,  0.5912309 ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.17312362,  0.70182687,  1.        ,\n",
       "         1.        ,  0.6859724 ,  0.37115428,  0.36314138,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.84122562,  0.37979941,  1.        ,  1.        ,  1.        ,\n",
       "         0.7615969 ,  1.        ,  1.        ,  1.        ,  0.45983037,\n",
       "         1.        ,  0.43329054,  1.        ,  0.53979297,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.78193989,\n",
       "         1.        ,  1.        ,  0.83232796,  0.84892063,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.40132964,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.dual_coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_simplified:\n",
    "    def __init__(self, C=1.0, gamma=0.1, tol=1e-3, max_iter=1200):\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.alphas = None\n",
    "        self.b = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.n_iter_ = 0\n",
    "\n",
    "    def rbf_kernel(self, x1, x2):\n",
    "      return np.exp(-self.gamma * (np.linalg.norm(x1 - x2) ** 2))\n",
    "    \n",
    "    def compute_error(self, n_samples, y, i, K):\n",
    "      err = self.b - y[i]\n",
    "      for j in range(n_samples):\n",
    "        err += self.alphas[j] * y[j] * K[j, i]\n",
    "      return err\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "      # random.seed(42)\n",
    "      n_samples, n_features = X.shape\n",
    "\n",
    "      K = np.zeros((n_samples, n_samples))\n",
    "      for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "          K[i,j] = self.rbf_kernel(X[i], X[j])\n",
    "      \n",
    "      self.alphas = np.zeros(n_samples)\n",
    "      self.b = 0\n",
    "      passes = 0\n",
    "      while passes < self.max_iter:\n",
    "        self.n_iter_ += 1\n",
    "        num_changed_alphas = 0\n",
    "        for i in range(n_samples):\n",
    "          Ei = np.sum(self.alphas*y*K[i,:]) + self.b - y[i]\n",
    "          # Ei = self.compute_error(n_samples, y, i, K)\n",
    "          # Ei = np.sum(np.dot(np.dot(self.alphas, y), K[i,:])) +self.b - y[i]\n",
    "          # if abs(ei - Ei)>0.00001:\n",
    "          #   print(Ei, ei)\n",
    "          if (((y[i]*Ei < -self.tol) and (self.alphas[i] < self.C)) or ((y[i]*Ei>self.tol) and (self.alphas[i]>0))):\n",
    "            j = random.choice(list(range(i))+list(range(i+1,n_samples)))\n",
    "            Ej = np.sum(self.alphas*y*K[j, :]) + self.b - y[j]\n",
    "            # Ej = self.compute_error(n_samples, y, j, K)\n",
    "            # Ej = np.sum(np.dot(np.dot(self.alphas, y), K[j,:])) +self.b - y[j]\n",
    "            alpha_i_old = self.alphas[i]\n",
    "            alpha_j_old = self.alphas[j]\n",
    "            if (y[i]!=y[j]):\n",
    "              L = max(0, self.alphas[j] - self.alphas[i])\n",
    "              H = min(self.C, self.C + self.alphas[j] - self.alphas[i])\n",
    "            else:\n",
    "              L = max(0, self.alphas[i] + self.alphas[j] - self.C)\n",
    "              H = min(self.C, self.alphas[i] + self.alphas[j])\n",
    "            if L==H:\n",
    "              continue\n",
    "            eta = 2 * K[i,j] - K[i,i] - K[j,j]\n",
    "            if eta>=0:\n",
    "              continue\n",
    "            self.alphas[j] -= y[j]*(Ei-Ej)/eta\n",
    "            self.alphas[j] = max(L, min(H, self.alphas[j]))\n",
    "\n",
    "            if (abs(self.alphas[j] - alpha_j_old) < 0.00001):\n",
    "              continue\n",
    "            self.alphas[i] += y[i]*y[j]*(alpha_j_old - self.alphas[j])\n",
    "            \n",
    "            b1 = self.b - Ei - y[i]*(self.alphas[i]-alpha_i_old)*K[i,i] - y[j]*(self.alphas[j]-alpha_j_old)*K[i,j]\n",
    "            b2 = self.b - Ej - y[i]*(self.alphas[i]-alpha_i_old)*K[i,j] - y[j]*(self.alphas[j]-alpha_j_old)*K[j,j]\n",
    "            if (0 < self.alphas[i] and self.alphas[i]<self.C):\n",
    "              self.b = b1\n",
    "            elif (0<self.alphas[j] and self.alphas[j]<self.C):\n",
    "              self.b = b2\n",
    "            else:\n",
    "              self.b = (b1 + b2)/2\n",
    "            num_changed_alphas += 1\n",
    "        if (num_changed_alphas == 0):\n",
    "          passes+= 1\n",
    "        else:\n",
    "          passes = 0\n",
    "      sv_idx = (self.alphas>0)\n",
    "      self.sv_idx = sv_idx\n",
    "      self.alphas = self.alphas[sv_idx]\n",
    "      self.support_vectors = X[sv_idx]\n",
    "      self.support_vector_labels = y[sv_idx]\n",
    "    def predict(self, X):\n",
    "      y_pred = np.zeros(len(X))\n",
    "      for i in range(len(X)):\n",
    "        s = self.b\n",
    "        for alpha, sv_y, sv_x in zip(self.alphas, self.support_vector_labels, self.support_vectors):\n",
    "          s += alpha * sv_y * self.rbf_kernel(X[i], sv_x)\n",
    "        y_pred[i] = s\n",
    "      return np.sign(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5416666666666666"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVM_simplified(gamma = 1/(x_train.shape[1]*x_train.var()))\n",
    "svm.fit(x_train, y_train)\n",
    "pred = svm.predict(x_test)\n",
    "accuracy_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SVM():\n",
    "  def __init__(self,C=1.0):\n",
    "    # C error terms\n",
    "    self.C = C\n",
    "    self.w = 0\n",
    "    self.b = 0\n",
    "\n",
    "  # Hinge Loss Function / Calculation\n",
    "  def hingeloss(self, w, b, x, y):\n",
    "    # Regularizer term\n",
    "    reg = 0.5 * (w * w)\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "      # Optimization term\n",
    "      opt_term = y[i] * ((np.dot(w, x[i])) + b)\n",
    "\n",
    "      # calculating loss\n",
    "      loss = reg + self.C * max(0, 1-opt_term)\n",
    "    return loss[0][0]\n",
    "\n",
    "  def fit(self, X, Y, batch_size=100, learning_rate=0.001, epochs=1000):\n",
    "    # The number of features in X\n",
    "    number_of_features = X.shape[1]\n",
    "\n",
    "    # The number of Samples in X\n",
    "    number_of_samples = X.shape[0]\n",
    "\n",
    "    c = self.C\n",
    "\n",
    "    # Creating ids from 0 to number_of_samples - 1\n",
    "    ids = np.arange(number_of_samples)\n",
    "\n",
    "    # Shuffling the samples randomly\n",
    "    np.random.shuffle(ids)\n",
    "\n",
    "    # creating an array of zeros\n",
    "    w = np.zeros((1, number_of_features))\n",
    "    b = 0\n",
    "    losses = []\n",
    "\n",
    "    # Gradient Descent logic\n",
    "    for i in range(epochs):\n",
    "      # Calculating the Hinge Loss\n",
    "      l = self.hingeloss(w, b, X, Y)\n",
    "\n",
    "      # Appending all losses \n",
    "      losses.append(l)\n",
    "      \n",
    "      # Starting from 0 to the number of samples with batch_size as interval\n",
    "      for batch_initial in range(0, number_of_samples, batch_size):\n",
    "        gradw = 0\n",
    "        gradb = 0\n",
    "\n",
    "        for j in range(batch_initial, batch_initial + batch_size):\n",
    "          if j < number_of_samples:\n",
    "            x = ids[j]\n",
    "            ti = Y[x] * (np.dot(w, X[x].T) + b)\n",
    "\n",
    "            if ti > 1:\n",
    "              gradw += 0\n",
    "              gradb += 0\n",
    "            else:\n",
    "              # Calculating the gradients\n",
    "              #w.r.t w \n",
    "              gradw += c * Y[x] * X[x]\n",
    "              # w.r.t b\n",
    "              gradb += c * Y[x]\n",
    "\n",
    "        # Updating weights and bias\n",
    "        w = w - learning_rate * w + learning_rate * gradw\n",
    "        b = b + learning_rate * gradb\n",
    "\n",
    "    self.w = w\n",
    "    self.b = b\n",
    "\n",
    "    return self.w, self.b, losses\n",
    "        \n",
    "  def predict(self, X):\n",
    "    prediction = np.dot(X, self.w[0]) + self.b # w.x + b\n",
    "    return np.sign(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = SVM()\n",
    "svm.fit(x_train, y_train)\n",
    "pred = svm.predict(x_test)\n",
    "accuracy_score(pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
